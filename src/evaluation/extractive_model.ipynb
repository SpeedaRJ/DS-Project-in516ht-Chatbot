{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from datasets import *\n",
    "import numpy as np\n",
    "import pathlib as pl\n",
    "import torch\n",
    "import ast\n",
    "from haystack.document_stores import FAISSDocumentStore\n",
    "from haystack.nodes import DensePassageRetriever, TransformersReader\n",
    "from haystack.pipelines import Pipeline\n",
    "\n",
    "year = 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_store = FAISSDocumentStore.load(index_path=\"../app/document_store.faiss\", config_path=\"../app/document_store.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rjutr\\miniconda3\\envs\\project_ds_2\\lib\\site-packages\\torch\\_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "retriever = DensePassageRetriever(\n",
    "    document_store=document_store,\n",
    "    query_embedding_model=\"facebook/dpr-question_encoder-single-nq-base\",\n",
    "    passage_embedding_model=\"facebook/dpr-ctx_encoder-single-nq-base\",\n",
    "    use_gpu=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rjutr\\miniconda3\\envs\\project_ds_2\\lib\\site-packages\\torch\\_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "reader = TransformersReader(model_name_or_path=f\"..\\initial_moddeling\\distilbert-qa\\distilbert-nlb-qa-{year}\", use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline()\n",
    "pipe.add_node(component=retriever, name=\"Retriever\", inputs=[\"Query\"])\n",
    "pipe.add_node(component=reader, name=\"Model\", inputs=[\"Retriever\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/rjutr/.cache/huggingface/datasets/csv/default-e3048f1bd60b5c4e/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "Loading cached split indices for dataset at C:\\Users\\rjutr\\.cache\\huggingface\\datasets\\csv\\default-e3048f1bd60b5c4e\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-e96c5bd318352c3f.arrow and C:\\Users\\rjutr\\.cache\\huggingface\\datasets\\csv\\default-e3048f1bd60b5c4e\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-3f782815aab69336.arrow\n",
      "Loading cached processed dataset at C:\\Users\\rjutr\\.cache\\huggingface\\datasets\\csv\\default-e3048f1bd60b5c4e\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-0e58ef57ff25bb58.arrow\n",
      "Loading cached processed dataset at C:\\Users\\rjutr\\.cache\\huggingface\\datasets\\csv\\default-e3048f1bd60b5c4e\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-944b27bfd97247d4.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['index', 'question', 'context', 'answers', 'id'],\n",
       "    num_rows: 107\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_dataset('csv', data_files=f\"../data/clean/sustainability-report-{year}-squad-format.csv\",\n",
    "                    delimiter=\";\", split='train').train_test_split(test_size=0.3, shuffle=True, seed=42)\n",
    "\n",
    "data[\"test\"] = data[\"test\"].map(\n",
    "    lambda example: ast.literal_eval(example[\"answers\"]))\n",
    "data[\"test\"] = data[\"test\"].map(lambda example: {\"question\": example[\"question\"], \"context\": example[\"context\"], \"answers\": {\n",
    "                                  \"text\": example[\"text\"], \"answer_start\": example[\"answer_start\"]}})\n",
    "data[\"test\"].remove_columns([\"text\", \"answer_start\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device\n",
    "device = torch.device(\n",
    "    \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prediction function\n",
    "def inference_answer(model, question):\n",
    "    question = question\n",
    "    with torch.no_grad():\n",
    "        outputs = model.run(query=question, params={\"Model\": {\"top_k\": 3}})\n",
    "    return outputs[\"answers\"][0].answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rjutr\\miniconda3\\envs\\project_ds_2\\lib\\site-packages\\transformers\\pipelines\\base.py:1070: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "answer_pred = [inference_answer(pipe, data[\"test\"][\"question\"][idx]) for idx in range(data[\"test\"].shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_true = [data[\"test\"][\"answers\"][idx][\"text\"][0]\n",
    "               for idx in range(data[\"test\"].shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertscore = evaluate.load(\"bertscore\")\n",
    "squad_v2_metric = evaluate.load(\"squad_v2\")\n",
    "bleu = evaluate.load(\"bleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rjutr\\miniconda3\\envs\\project_ds_2\\lib\\site-packages\\torch\\_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.9175579792985292, Precision: 0.9185575748158392, Recall: 0.9168313507721803\n"
     ]
    }
   ],
   "source": [
    "results = bertscore.compute(predictions=answer_pred,\n",
    "                            references=answer_true, lang=\"en\")\n",
    "# Embeddings bases evaluation\n",
    "print(\n",
    "    f\"F1: {np.array(results['f1']).mean()}, Precision: {np.array(results['precision']).mean()}, Recall: {np.array(results['recall']).mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact': 31.77570093457944,\n",
       " 'f1': 36.37870436935857,\n",
       " 'total': 107,\n",
       " 'HasAns_exact': 31.77570093457944,\n",
       " 'HasAns_f1': 36.37870436935857,\n",
       " 'HasAns_total': 107,\n",
       " 'best_exact': 31.77570093457944,\n",
       " 'best_exact_thresh': 0.0,\n",
       " 'best_f1': 36.37870436935857,\n",
       " 'best_f1_thresh': 0.0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "references = [{\"answers\": {\"answer_start\": [answer[\"answer_start\"][0]], \"text\": [\n",
    "    answer[\"text\"][0]]}, \"id\": str(id)} for id, answer in zip(data[\"test\"][\"id\"], data[\"test\"][\"answers\"])]\n",
    "predictions = [{\"id\": str(id), \"prediction_text\": answer, \"no_answer_probability\": 0.}\n",
    "               for id, answer in zip(data[\"test\"][\"id\"], answer_pred)]\n",
    "results = squad_v2_metric.compute(\n",
    "    predictions=predictions, references=references)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.16736303030434813,\n",
       " 'precisions': [0.31225296442687744,\n",
       "  0.1917808219178082,\n",
       "  0.14606741573033707,\n",
       "  0.12903225806451613],\n",
       " 'brevity_penalty': 0.9131007162822622,\n",
       " 'length_ratio': 0.9166666666666666,\n",
       " 'translation_length': 253,\n",
       " 'reference_length': 276}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "references = [[answer] for answer in answer_true]\n",
    "# N-Gram based evaluation\n",
    "results = bleu.compute(predictions=answer_pred, references=references)\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_ds_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

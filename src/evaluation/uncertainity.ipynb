{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, set_seed\n",
    "from datasets import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import ast\n",
    "import evaluate\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_models_path = '../data/models/T5/SE'\n",
    "\n",
    "results = pd.DataFrame(columns=['Model', 'Data Type', 'Bert.Precision', 'Bert.Recall', 'Bert.F1', 'BLEU'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertscore = evaluate.load(\"bertscore\")\n",
    "bleu = evaluate.load(\"bleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(question, context, tokenizer, model):\n",
    "    input_text = \"question: %s  context: %s\" % (question, context)\n",
    "    features = tokenizer([input_text], return_tensors='pt')\n",
    "\n",
    "    output = model.generate(input_ids=features['input_ids'], attention_mask=features['attention_mask'])\n",
    "\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = os.listdir(local_models_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "handwritten = [model for model in models if 'handwritten' in model]\n",
    "rest = [model for model in models if 'handwritten' not in model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model t5-small-finetuned-squadv2-finetuned-NLB-QA-2042-full_combined-42...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/Luka/.cache/huggingface/datasets/csv/default-a6bdc04297c1a3c2/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "Loading cached split indices for dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-a6bdc04297c1a3c2\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-4c58d6607e064ca8.arrow and C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-a6bdc04297c1a3c2\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-6ec98b5ad96d608a.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-a6bdc04297c1a3c2\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-b60d97fbf617fa23.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-a6bdc04297c1a3c2\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-da9287d5acd53d81.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-a6bdc04297c1a3c2\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-0fead63929b725db.arrow\n",
      "c:\\Users\\Luka\\miniconda3\\envs\\project_ds\\lib\\site-packages\\transformers\\generation\\utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model t5-small-finetuned-squadv2-finetuned-NLB-QA-2042-full_combined-43...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/Luka/.cache/huggingface/datasets/csv/default-a6bdc04297c1a3c2/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "Loading cached split indices for dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-a6bdc04297c1a3c2\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-979a8191f09e1035.arrow and C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-a6bdc04297c1a3c2\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-82741fa36c858cca.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-a6bdc04297c1a3c2\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-d3b0fec5fe205d53.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-a6bdc04297c1a3c2\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-c40597fb67e48763.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-a6bdc04297c1a3c2\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-1d66e77a772485c7.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model t5-small-finetuned-squadv2-finetuned-NLB-QA-2042-full_combined-44...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/Luka/.cache/huggingface/datasets/csv/default-a6bdc04297c1a3c2/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "Loading cached split indices for dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-a6bdc04297c1a3c2\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-69828c5ed2068a89.arrow and C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-a6bdc04297c1a3c2\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-175c54e2b0a2e46e.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-a6bdc04297c1a3c2\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-b56acd9e2fa0433c.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-a6bdc04297c1a3c2\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-a827241191e10312.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-a6bdc04297c1a3c2\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-c35ffc56de60304d.arrow\n",
      "c:\\Users\\Luka\\miniconda3\\envs\\project_ds\\lib\\site-packages\\transformers\\generation\\utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model t5-small-finetuned-squadv2-finetuned-NLB-QA-2042-full_combined-45...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/Luka/.cache/huggingface/datasets/csv/default-a6bdc04297c1a3c2/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "Loading cached split indices for dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-a6bdc04297c1a3c2\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-e397db2e12898aba.arrow and C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-a6bdc04297c1a3c2\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-4a72975e1157b2ba.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-a6bdc04297c1a3c2\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-86c06e6f249d16cd.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-a6bdc04297c1a3c2\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-2f1a20fbc2165910.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-a6bdc04297c1a3c2\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-8c5b1cba4c9c153e.arrow\n",
      "c:\\Users\\Luka\\miniconda3\\envs\\project_ds\\lib\\site-packages\\transformers\\generation\\utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model t5-small-finetuned-squadv2-finetuned-NLB-QA-2042-full_combined-46...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/Luka/.cache/huggingface/datasets/csv/default-a6bdc04297c1a3c2/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "Loading cached split indices for dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-a6bdc04297c1a3c2\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-dd966a71d4e46a5d.arrow and C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-a6bdc04297c1a3c2\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-19648ebf80580d50.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-a6bdc04297c1a3c2\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-27d8357ba49892aa.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-a6bdc04297c1a3c2\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-8c27d6b8b6d52cde.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-a6bdc04297c1a3c2\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-720b022aef6fe26a.arrow\n",
      "c:\\Users\\Luka\\miniconda3\\envs\\project_ds\\lib\\site-packages\\transformers\\generation\\utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model t5-small-finetuned-squadv2-finetuned-NLB-QA-2042-full_combined-47...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/Luka/.cache/huggingface/datasets/csv/default-a6bdc04297c1a3c2/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "Loading cached split indices for dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-a6bdc04297c1a3c2\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-9a4755ed0b32b928.arrow and C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-a6bdc04297c1a3c2\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-096fdebce4cb73e5.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-a6bdc04297c1a3c2\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-9254f3044c5d31d7.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-a6bdc04297c1a3c2\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-4d7a7dc13a29d45f.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-a6bdc04297c1a3c2\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-d6c1dcf26e63cf76.arrow\n",
      "c:\\Users\\Luka\\miniconda3\\envs\\project_ds\\lib\\site-packages\\transformers\\generation\\utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model t5-small-finetuned-squadv2-finetuned-NLB-QA-2042-full_combined-48...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/Luka/.cache/huggingface/datasets/csv/default-a6bdc04297c1a3c2/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "Loading cached split indices for dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-a6bdc04297c1a3c2\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-5e4964374df4b312.arrow and C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-a6bdc04297c1a3c2\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-0f6b89900769f2cd.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-a6bdc04297c1a3c2\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-1d5bd1c5b7e0b415.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-a6bdc04297c1a3c2\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-30df0b49c1d5ed5d.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-a6bdc04297c1a3c2\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-ed6b8e1da5fb1774.arrow\n",
      "c:\\Users\\Luka\\miniconda3\\envs\\project_ds\\lib\\site-packages\\transformers\\generation\\utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model t5-small-finetuned-squadv2-finetuned-NLB-QA-2042-full_combined-49...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/Luka/.cache/huggingface/datasets/csv/default-a6bdc04297c1a3c2/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "Loading cached split indices for dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-a6bdc04297c1a3c2\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-d34a0b1b11ab4a95.arrow and C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-a6bdc04297c1a3c2\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-b665426520fe3b88.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-a6bdc04297c1a3c2\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-dbd4ef42eb1f3d16.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-a6bdc04297c1a3c2\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-c01f07850c451399.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-a6bdc04297c1a3c2\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-bb9d33db5b1ba871.arrow\n",
      "c:\\Users\\Luka\\miniconda3\\envs\\project_ds\\lib\\site-packages\\transformers\\generation\\utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model t5-small-finetuned-squadv2-finetuned-NLB-QA-2042-full_combined-50...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/Luka/.cache/huggingface/datasets/csv/default-a6bdc04297c1a3c2/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "Loading cached split indices for dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-a6bdc04297c1a3c2\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-2d1d36e1fafcd624.arrow and C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-a6bdc04297c1a3c2\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-e5a3431c3645914a.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-a6bdc04297c1a3c2\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-ab781c79497628ff.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-a6bdc04297c1a3c2\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-39c0a9da5a394f05.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-a6bdc04297c1a3c2\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-c48ae426a9439bfd.arrow\n",
      "c:\\Users\\Luka\\miniconda3\\envs\\project_ds\\lib\\site-packages\\transformers\\generation\\utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model t5-small-finetuned-squadv2-finetuned-NLB-QA-2042-full_combined-51...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/Luka/.cache/huggingface/datasets/csv/default-a6bdc04297c1a3c2/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "Loading cached split indices for dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-a6bdc04297c1a3c2\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-3bdefb8b746cf12d.arrow and C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-a6bdc04297c1a3c2\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-d0ac775eb9473ec2.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-a6bdc04297c1a3c2\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-ba63ee62d3e2d4d2.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-a6bdc04297c1a3c2\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-2ebfcc63cf8ec15c.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-a6bdc04297c1a3c2\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-71fa53fd81d5e97f.arrow\n",
      "c:\\Users\\Luka\\miniconda3\\envs\\project_ds\\lib\\site-packages\\transformers\\generation\\utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = None\n",
    "tokenizer = None\n",
    "\n",
    "for model in rest:\n",
    "    model_data = model.split(\"-\")\n",
    "\n",
    "    print(f\"Evaluating model {model}...\")\n",
    "    model_name = f\"{local_models_path}/{model}\"\n",
    "    seed = model_data[-1]\n",
    "    set_seed(int(seed))\n",
    "\n",
    "    data_2020_2022 = load_dataset('csv', data_files=\"../data/clean/sustainability-report-2042-squad-format.csv\",\n",
    "                                delimiter=\";\", split=\"train\").train_test_split(test_size=0.3, shuffle=True, seed=int(seed))\n",
    "\n",
    "    # Reformat the train and test set such as they adhere to the SQuAD format (reading from cvs loads strings not objects as expected)\n",
    "    data_2020_2022[\"test\"] = data_2020_2022[\"test\"].map(\n",
    "        lambda example: ast.literal_eval(example[\"answers\"]))\n",
    "    data_2020_2022[\"test\"] = data_2020_2022[\"test\"].map(lambda example: {\"question\": example[\"question\"], \"context\": example[\"context\"], \"answers\": {\n",
    "                                    \"text\": example[\"text\"], \"answer_start\": example[\"answer_start\"]}})\n",
    "    # replace all \"\\n\" with \" \" in the context, answers and questions\n",
    "    data_2020_2022[\"test\"] = data_2020_2022[\"test\"].map(lambda example: {\"question\": example[\"question\"].replace(\"\\n\", \" \"), \"context\": example[\"context\"].replace(\"\\n\", \" \"), \"answers\": {\n",
    "                                    \"text\": [example[\"answers\"][\"text\"][0].replace(\"\\n\", \" \")], \"answer_start\": example[\"answers\"][\"answer_start\"]}})\n",
    "    data_2020_2022[\"test\"] = data_2020_2022[\"test\"].remove_columns([\"text\", \"answer_start\"])\n",
    "    # get ground truth answers\n",
    "    test_data_2020_2022 = data_2020_2022[\"test\"]\n",
    "    gt_answers_2020_2022 = [temp[\"answers\"][\"text\"][0] for temp in test_data_2020_2022]\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, local_files_only=True)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name, local_files_only=True)\n",
    "\n",
    "    answers_2020_2022 = [get_answer(question, context, tokenizer, model) for question, context in zip(test_data_2020_2022[\"question\"], test_data_2020_2022[\"context\"])]\n",
    "\n",
    "    bert_results_2020_2022 = bertscore.compute(predictions=answers_2020_2022, references=gt_answers_2020_2022, lang=\"en\")\n",
    "    bleu_results_2020_2022 = bleu.compute(predictions=answers_2020_2022, references=gt_answers_2020_2022)\n",
    "\n",
    "    results.loc[len(results)] = ['t5-small', '2020-2022', np.array(bert_results_2020_2022['precision']).mean(), np.array(bert_results_2020_2022['recall']).mean(), np.array(bert_results_2020_2022['f1']).mean(), bleu_results_2020_2022['bleu']]\n",
    "    \n",
    "    del tokenizer\n",
    "    del model\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model t5-small-finetuned-squadv2-finetuned-NLB-QA-2022-handwritten-42...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/Luka/.cache/huggingface/datasets/csv/default-853b320bab41342e/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "Loading cached split indices for dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-853b320bab41342e\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-690d708f98f9a3b4.arrow and C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-853b320bab41342e\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-b3c2cbaa1563558c.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-853b320bab41342e\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-227bc1b6d7ce66b1.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-853b320bab41342e\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-ea73fd3146b3fd5a.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-853b320bab41342e\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-51580e58cadacd1e.arrow\n",
      "c:\\Users\\Luka\\miniconda3\\envs\\project_ds\\lib\\site-packages\\transformers\\generation\\utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model t5-small-finetuned-squadv2-finetuned-NLB-QA-2022-handwritten-43...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/Luka/.cache/huggingface/datasets/csv/default-853b320bab41342e/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "Loading cached split indices for dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-853b320bab41342e\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-c2e5e8fd88c0fa07.arrow and C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-853b320bab41342e\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-1c0de4013f019b4a.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-853b320bab41342e\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-5576838a6f55ab89.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-853b320bab41342e\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-d0041066c4ca1a8f.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-853b320bab41342e\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-1129e3e4f9123343.arrow\n",
      "c:\\Users\\Luka\\miniconda3\\envs\\project_ds\\lib\\site-packages\\transformers\\generation\\utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model t5-small-finetuned-squadv2-finetuned-NLB-QA-2022-handwritten-44...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/Luka/.cache/huggingface/datasets/csv/default-853b320bab41342e/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "Loading cached split indices for dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-853b320bab41342e\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-fabca9674a4c716f.arrow and C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-853b320bab41342e\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-bf85bfba9b359153.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-853b320bab41342e\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-989508871473c792.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-853b320bab41342e\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-19b04c575b6b7983.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-853b320bab41342e\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-ad773f6c0b9df447.arrow\n",
      "c:\\Users\\Luka\\miniconda3\\envs\\project_ds\\lib\\site-packages\\transformers\\generation\\utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model t5-small-finetuned-squadv2-finetuned-NLB-QA-2022-handwritten-45...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/Luka/.cache/huggingface/datasets/csv/default-853b320bab41342e/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "Loading cached split indices for dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-853b320bab41342e\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-586eb591e6221fb3.arrow and C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-853b320bab41342e\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-be69ae01d296ddeb.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-853b320bab41342e\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-10874be8e2f844c5.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-853b320bab41342e\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-2c4aa7578c5e9476.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-853b320bab41342e\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-3365c2d439ab6dc5.arrow\n",
      "c:\\Users\\Luka\\miniconda3\\envs\\project_ds\\lib\\site-packages\\transformers\\generation\\utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model t5-small-finetuned-squadv2-finetuned-NLB-QA-2022-handwritten-46...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/Luka/.cache/huggingface/datasets/csv/default-853b320bab41342e/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "Loading cached split indices for dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-853b320bab41342e\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-a8dcff8eb83cfdbd.arrow and C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-853b320bab41342e\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-a95cc6e60dbb55db.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-853b320bab41342e\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-ea620f7818b1f48d.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-853b320bab41342e\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-ffa6b8d6e6712ab7.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-853b320bab41342e\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-b4dbd57f2d02a6f6.arrow\n",
      "c:\\Users\\Luka\\miniconda3\\envs\\project_ds\\lib\\site-packages\\transformers\\generation\\utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model t5-small-finetuned-squadv2-finetuned-NLB-QA-2022-handwritten-47...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/Luka/.cache/huggingface/datasets/csv/default-853b320bab41342e/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "Loading cached split indices for dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-853b320bab41342e\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-b2bd1721f315fbc0.arrow and C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-853b320bab41342e\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-726127568a5514f7.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-853b320bab41342e\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-0a35176cf5350ffb.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-853b320bab41342e\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-d14f33d5582852bc.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-853b320bab41342e\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-90d398acb4001a9d.arrow\n",
      "c:\\Users\\Luka\\miniconda3\\envs\\project_ds\\lib\\site-packages\\transformers\\generation\\utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model t5-small-finetuned-squadv2-finetuned-NLB-QA-2022-handwritten-48...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/Luka/.cache/huggingface/datasets/csv/default-853b320bab41342e/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "Loading cached split indices for dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-853b320bab41342e\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-d7a61e5f68c4870e.arrow and C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-853b320bab41342e\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-dc27bbd6f551d362.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-853b320bab41342e\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-5de91d534e00cfe4.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-853b320bab41342e\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-043b69d21f2b9fd1.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-853b320bab41342e\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-f81cd2c75940e468.arrow\n",
      "c:\\Users\\Luka\\miniconda3\\envs\\project_ds\\lib\\site-packages\\transformers\\generation\\utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model t5-small-finetuned-squadv2-finetuned-NLB-QA-2022-handwritten-49...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/Luka/.cache/huggingface/datasets/csv/default-853b320bab41342e/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "Loading cached split indices for dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-853b320bab41342e\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-f127e20bfb0e4faf.arrow and C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-853b320bab41342e\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-be2b029fbc7bc3bf.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-853b320bab41342e\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-9f4c17020c08c45c.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-853b320bab41342e\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-871d9beeb4c25ea4.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-853b320bab41342e\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-79dd0f91d0a71d63.arrow\n",
      "c:\\Users\\Luka\\miniconda3\\envs\\project_ds\\lib\\site-packages\\transformers\\generation\\utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model t5-small-finetuned-squadv2-finetuned-NLB-QA-2022-handwritten-50...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/Luka/.cache/huggingface/datasets/csv/default-853b320bab41342e/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "Loading cached split indices for dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-853b320bab41342e\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-31f7281be09e829a.arrow and C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-853b320bab41342e\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-8dcdbdb5507f9b95.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-853b320bab41342e\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-966f5da43e073ed3.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-853b320bab41342e\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-ed8d1e5688f5f911.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-853b320bab41342e\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-6cbd30d54f214bf2.arrow\n",
      "c:\\Users\\Luka\\miniconda3\\envs\\project_ds\\lib\\site-packages\\transformers\\generation\\utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model t5-small-finetuned-squadv2-finetuned-NLB-QA-2022-handwritten-51...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/Luka/.cache/huggingface/datasets/csv/default-853b320bab41342e/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "Loading cached split indices for dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-853b320bab41342e\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-18648557ffcf97ce.arrow and C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-853b320bab41342e\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-c4b56581efe1a940.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-853b320bab41342e\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-b679643ee972cfa3.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-853b320bab41342e\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-e76297182368c96b.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-853b320bab41342e\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-37d97d33dc3a7f18.arrow\n",
      "c:\\Users\\Luka\\miniconda3\\envs\\project_ds\\lib\\site-packages\\transformers\\generation\\utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = None\n",
    "model = None\n",
    "\n",
    "for model in handwritten:\n",
    "    model_data = model.split(\"-\")\n",
    "\n",
    "    print(f\"Evaluating model {model}...\")\n",
    "    model_name = f\"{local_models_path}/{model}\"\n",
    "    seed = model_data[-1]\n",
    "    set_seed(int(seed))\n",
    "\n",
    "    data_2022_handwritten = load_dataset('csv', data_files=f\"../data/clean/QA_SR_2022_Expert-squad-format.csv\",\n",
    "                                        delimiter=\";\", split='train').train_test_split(test_size=0.3, shuffle=True, seed=int(seed))\n",
    "\n",
    "    # Reformat the train and test set such as they adhere to the SQuAD format (reading from cvs loads strings not objects as expected)\n",
    "    data_2022_handwritten[\"test\"] = data_2022_handwritten[\"test\"].map(\n",
    "        lambda example: ast.literal_eval(example[\"answers\"]))\n",
    "    data_2022_handwritten[\"test\"] = data_2022_handwritten[\"test\"].map(lambda example: {\"question\": example[\"question\"], \"context\": example[\"context\"], \"answers\": {\n",
    "                                    \"text\": example[\"text\"], \"answer_start\": example[\"answer_start\"]}})\n",
    "    # replace all \"\\n\" with \" \" in the context, answers and questions\n",
    "    data_2022_handwritten[\"test\"] = data_2022_handwritten[\"test\"].map(lambda example: {\"question\": example[\"question\"].replace(\"\\n\", \" \"), \"context\": example[\"context\"].replace(\"\\n\", \" \"), \"answers\": {\n",
    "                                    \"text\": [example[\"answers\"][\"text\"][0].replace(\"\\n\", \" \")], \"answer_start\": example[\"answers\"][\"answer_start\"]}})\n",
    "    data_2022_handwritten[\"test\"] = data_2022_handwritten[\"test\"].remove_columns([\"text\", \"answer_start\"])\n",
    "\n",
    "    test_data_2022_handwritten = data_2022_handwritten[\"test\"]\n",
    "    gt_answers_2022_handwritten = [temp[\"answers\"][\"text\"][0] for temp in test_data_2022_handwritten]\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, local_files_only=True)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name, local_files_only=True)\n",
    "\n",
    "    answers_2022_handwritten = [get_answer(question, context, tokenizer, model) for question, context in zip(test_data_2022_handwritten[\"question\"], test_data_2022_handwritten[\"context\"])]\n",
    "\n",
    "    bert_results_2022_handwritten = bertscore.compute(predictions=answers_2022_handwritten, references=gt_answers_2022_handwritten, lang=\"en\")\n",
    "    bleu_results_2022_handwritten = bleu.compute(predictions=answers_2022_handwritten, references=gt_answers_2022_handwritten)\n",
    "\n",
    "    \n",
    "    results.loc[len(results)] = ['t5-small', 'handwritten', np.array(bert_results_2022_handwritten['precision']).mean(), np.array(bert_results_2022_handwritten['recall']).mean(), np.array(bert_results_2022_handwritten['f1']).mean(), bleu_results_2022_handwritten['bleu']]\n",
    "     \n",
    "    del tokenizer\n",
    "    del model\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Data Type</th>\n",
       "      <th>Bert.Precision</th>\n",
       "      <th>Bert.Recall</th>\n",
       "      <th>Bert.F1</th>\n",
       "      <th>BLEU</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t5-small</td>\n",
       "      <td>2020-2022</td>\n",
       "      <td>0.968892</td>\n",
       "      <td>0.981904</td>\n",
       "      <td>0.975069</td>\n",
       "      <td>0.543063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t5-small</td>\n",
       "      <td>2020-2022</td>\n",
       "      <td>0.972492</td>\n",
       "      <td>0.979039</td>\n",
       "      <td>0.975480</td>\n",
       "      <td>0.531009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t5-small</td>\n",
       "      <td>2020-2022</td>\n",
       "      <td>0.963853</td>\n",
       "      <td>0.972613</td>\n",
       "      <td>0.967849</td>\n",
       "      <td>0.578645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t5-small</td>\n",
       "      <td>2020-2022</td>\n",
       "      <td>0.973157</td>\n",
       "      <td>0.980202</td>\n",
       "      <td>0.976383</td>\n",
       "      <td>0.614047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t5-small</td>\n",
       "      <td>2020-2022</td>\n",
       "      <td>0.966682</td>\n",
       "      <td>0.973360</td>\n",
       "      <td>0.969763</td>\n",
       "      <td>0.562466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>t5-small</td>\n",
       "      <td>2020-2022</td>\n",
       "      <td>0.973078</td>\n",
       "      <td>0.982073</td>\n",
       "      <td>0.977263</td>\n",
       "      <td>0.626648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>t5-small</td>\n",
       "      <td>2020-2022</td>\n",
       "      <td>0.974937</td>\n",
       "      <td>0.978690</td>\n",
       "      <td>0.976623</td>\n",
       "      <td>0.617954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>t5-small</td>\n",
       "      <td>2020-2022</td>\n",
       "      <td>0.970898</td>\n",
       "      <td>0.978491</td>\n",
       "      <td>0.974389</td>\n",
       "      <td>0.570040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>t5-small</td>\n",
       "      <td>2020-2022</td>\n",
       "      <td>0.966904</td>\n",
       "      <td>0.977225</td>\n",
       "      <td>0.971779</td>\n",
       "      <td>0.555747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>t5-small</td>\n",
       "      <td>2020-2022</td>\n",
       "      <td>0.967508</td>\n",
       "      <td>0.974424</td>\n",
       "      <td>0.970657</td>\n",
       "      <td>0.573230</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Model  Data Type  Bert.Precision  Bert.Recall   Bert.F1      BLEU\n",
       "0  t5-small  2020-2022        0.968892     0.981904  0.975069  0.543063\n",
       "1  t5-small  2020-2022        0.972492     0.979039  0.975480  0.531009\n",
       "2  t5-small  2020-2022        0.963853     0.972613  0.967849  0.578645\n",
       "3  t5-small  2020-2022        0.973157     0.980202  0.976383  0.614047\n",
       "4  t5-small  2020-2022        0.966682     0.973360  0.969763  0.562466\n",
       "5  t5-small  2020-2022        0.973078     0.982073  0.977263  0.626648\n",
       "6  t5-small  2020-2022        0.974937     0.978690  0.976623  0.617954\n",
       "7  t5-small  2020-2022        0.970898     0.978491  0.974389  0.570040\n",
       "8  t5-small  2020-2022        0.966904     0.977225  0.971779  0.555747\n",
       "9  t5-small  2020-2022        0.967508     0.974424  0.970657  0.573230"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Data Type</th>\n",
       "      <th>Bert.Precision</th>\n",
       "      <th>Bert.Recall</th>\n",
       "      <th>Bert.F1</th>\n",
       "      <th>BLEU</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>t5-small</td>\n",
       "      <td>handwritten</td>\n",
       "      <td>0.911456</td>\n",
       "      <td>0.878175</td>\n",
       "      <td>0.894139</td>\n",
       "      <td>0.221080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>t5-small</td>\n",
       "      <td>handwritten</td>\n",
       "      <td>0.930275</td>\n",
       "      <td>0.894927</td>\n",
       "      <td>0.911898</td>\n",
       "      <td>0.293840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>t5-small</td>\n",
       "      <td>handwritten</td>\n",
       "      <td>0.911231</td>\n",
       "      <td>0.876724</td>\n",
       "      <td>0.893213</td>\n",
       "      <td>0.114651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>t5-small</td>\n",
       "      <td>handwritten</td>\n",
       "      <td>0.920100</td>\n",
       "      <td>0.893000</td>\n",
       "      <td>0.905997</td>\n",
       "      <td>0.110058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>t5-small</td>\n",
       "      <td>handwritten</td>\n",
       "      <td>0.928027</td>\n",
       "      <td>0.900291</td>\n",
       "      <td>0.913582</td>\n",
       "      <td>0.243488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>t5-small</td>\n",
       "      <td>handwritten</td>\n",
       "      <td>0.924586</td>\n",
       "      <td>0.893090</td>\n",
       "      <td>0.908213</td>\n",
       "      <td>0.185495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>t5-small</td>\n",
       "      <td>handwritten</td>\n",
       "      <td>0.936590</td>\n",
       "      <td>0.899866</td>\n",
       "      <td>0.917407</td>\n",
       "      <td>0.308283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>t5-small</td>\n",
       "      <td>handwritten</td>\n",
       "      <td>0.901736</td>\n",
       "      <td>0.862724</td>\n",
       "      <td>0.881626</td>\n",
       "      <td>0.193429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>t5-small</td>\n",
       "      <td>handwritten</td>\n",
       "      <td>0.906286</td>\n",
       "      <td>0.875989</td>\n",
       "      <td>0.890618</td>\n",
       "      <td>0.128802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>t5-small</td>\n",
       "      <td>handwritten</td>\n",
       "      <td>0.908682</td>\n",
       "      <td>0.873197</td>\n",
       "      <td>0.890186</td>\n",
       "      <td>0.166017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Model    Data Type  Bert.Precision  Bert.Recall   Bert.F1      BLEU\n",
       "10  t5-small  handwritten        0.911456     0.878175  0.894139  0.221080\n",
       "11  t5-small  handwritten        0.930275     0.894927  0.911898  0.293840\n",
       "12  t5-small  handwritten        0.911231     0.876724  0.893213  0.114651\n",
       "13  t5-small  handwritten        0.920100     0.893000  0.905997  0.110058\n",
       "14  t5-small  handwritten        0.928027     0.900291  0.913582  0.243488\n",
       "15  t5-small  handwritten        0.924586     0.893090  0.908213  0.185495\n",
       "16  t5-small  handwritten        0.936590     0.899866  0.917407  0.308283\n",
       "17  t5-small  handwritten        0.901736     0.862724  0.881626  0.193429\n",
       "18  t5-small  handwritten        0.906286     0.875989  0.890618  0.128802\n",
       "19  t5-small  handwritten        0.908682     0.873197  0.890186  0.166017"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_2020_2022 = results[results['Data Type'] == '2020-2022']\n",
    "df_handwritten = results[results['Data Type'] == 'handwritten']\n",
    "display(df_2020_2022)\n",
    "display(df_handwritten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Luka\\AppData\\Local\\Temp\\ipykernel_13708\\3485297047.py:2: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_2020_2022_mean = df_2020_2022.mean(axis=0)\n",
      "C:\\Users\\Luka\\AppData\\Local\\Temp\\ipykernel_13708\\3485297047.py:3: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_2020_2022_sem = df_2020_2022.sem(axis=0)\n",
      "C:\\Users\\Luka\\AppData\\Local\\Temp\\ipykernel_13708\\3485297047.py:4: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_2020_2022_std = df_2020_2022.std(axis=0)\n",
      "C:\\Users\\Luka\\AppData\\Local\\Temp\\ipykernel_13708\\3485297047.py:6: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_handwritten_mean = df_handwritten.mean(axis=0)\n",
      "C:\\Users\\Luka\\AppData\\Local\\Temp\\ipykernel_13708\\3485297047.py:7: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_handwritten_sem = df_handwritten.sem(axis=0)\n",
      "C:\\Users\\Luka\\AppData\\Local\\Temp\\ipykernel_13708\\3485297047.py:8: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_handwritten_std = df_handwritten.std(axis=0)\n"
     ]
    }
   ],
   "source": [
    "# get average results and standard error\n",
    "df_2020_2022_mean = df_2020_2022.mean(axis=0)\n",
    "df_2020_2022_sem = df_2020_2022.sem(axis=0)\n",
    "df_2020_2022_std = df_2020_2022.std(axis=0)\n",
    "\n",
    "df_handwritten_mean = df_handwritten.mean(axis=0)\n",
    "df_handwritten_sem = df_handwritten.sem(axis=0)\n",
    "df_handwritten_std = df_handwritten.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-2022\n",
      "Mean\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Bert.Precision    0.969840\n",
       "Bert.Recall       0.977802\n",
       "Bert.F1           0.973526\n",
       "BLEU              0.577285\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Error\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Bert.Precision    0.001137\n",
       "Bert.Recall       0.001065\n",
       "Bert.F1           0.001035\n",
       "BLEU              0.010279\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Deviation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Bert.Precision    0.003596\n",
       "Bert.Recall       0.003367\n",
       "Bert.F1           0.003272\n",
       "BLEU              0.032505\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HANDWRITTEN\n",
      "Mean\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Bert.Precision    0.917897\n",
       "Bert.Recall       0.884798\n",
       "Bert.F1           0.900688\n",
       "BLEU              0.196514\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Error\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Bert.Precision    0.003684\n",
       "Bert.Recall       0.004102\n",
       "Bert.F1           0.003844\n",
       "BLEU              0.022260\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Deviation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Bert.Precision    0.011651\n",
       "Bert.Recall       0.012971\n",
       "Bert.F1           0.012157\n",
       "BLEU              0.070393\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display results\n",
    "print(\"2020-2022\")\n",
    "print(\"Mean\")\n",
    "display(df_2020_2022_mean)\n",
    "print(\"Standard Error\")\n",
    "display(df_2020_2022_sem)\n",
    "print(\"Standard Deviation\")\n",
    "display(df_2020_2022_std)\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"HANDWRITTEN\")\n",
    "print(\"Mean\")\n",
    "display(df_handwritten_mean)\n",
    "print(\"Standard Error\")\n",
    "display(df_handwritten_sem)\n",
    "print(\"Standard Deviation\")\n",
    "display(df_handwritten_std)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_ds_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import evaluate\n",
    "from datasets import *\n",
    "import numpy as np\n",
    "import pathlib as pl\n",
    "import pandas as pd\n",
    "import torch\n",
    "import ast\n",
    "import os\n",
    "from haystack.document_stores import FAISSDocumentStore\n",
    "from haystack.nodes import DensePassageRetriever, Seq2SeqGenerator\n",
    "from haystack.nodes.answer_generator.transformers import _BartEli5Converter\n",
    "from haystack.pipelines import Pipeline\n",
    "\n",
    "# Year identifier\n",
    "YEAR = 2042\n",
    "\n",
    "# Standard model path\n",
    "MODEL_PATH = \"../../models/T5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_data = load_dataset('csv', data_files=f\"../../data/clean/squad/sustainability-report-{YEAR}-squad-format.csv\",\n",
    "                    delimiter=\";\", split='train').train_test_split(test_size=0.3, shuffle=True, seed=42)\n",
    "# Reformat the train and test set such as they adhere to the SQuAD format (reading from cvs loads strings not objects as expected)\n",
    "generated_data[\"test\"] = generated_data[\"test\"].map(\n",
    "    lambda example: ast.literal_eval(example[\"answers\"]))\n",
    "# Replace all \"\\n\" with \" \" in the context, answers and questions\n",
    "generated_data[\"test\"] = generated_data[\"test\"].map(lambda example: {\"question\": example[\"question\"], \"context\": example[\"context\"], \"answers\": {\n",
    "                                  \"text\": example[\"text\"], \"answer_start\": example[\"answer_start\"]}})\n",
    "generated_data[\"test\"] = generated_data[\"test\"].map(lambda example: {\"question\": example[\"question\"].replace(\"\\n\", \" \"), \"context\": example[\"context\"].replace(\"\\n\", \" \"), \"answers\": {\n",
    "                                \"text\": [example[\"answers\"][\"text\"][0].replace(\"\\n\", \" \")], \"answer_start\": example[\"answers\"][\"answer_start\"]}})\n",
    "generated_data[\"test\"].remove_columns([\"text\", \"answer_start\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metrics\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "bleu = evaluate.load(\"bleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load document Store\n",
    "document_store = FAISSDocumentStore.load(index_path=\"document_store.faiss\", config_path=\"document_store.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load non fine-tuned retriever\n",
    "retriever = DensePassageRetriever(\n",
    "    document_store=document_store,\n",
    "    query_embedding_model=\"facebook/dpr-question_encoder-single-nq-base\",\n",
    "    passage_embedding_model=\"facebook/dpr-ctx_encoder-single-nq-base\",\n",
    "    use_gpu=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device\n",
    "device = torch.device(\n",
    "    \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Define the prediction function\n",
    "def inference_answer(model, question):\n",
    "    question = question\n",
    "    with torch.no_grad():\n",
    "        outputs = model.run(query=question, params={\"Model\": {\"top_k\": 3}})\n",
    "    return outputs[\"answers\"][0].answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model names\n",
    "models = os.listdir(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boilerplate results\n",
    "results = {\n",
    "    \"Model\": [],\n",
    "    \"DPR-ft\": [],\n",
    "    \"Data\": [],\n",
    "    \"Bert.Precision\": [],\n",
    "    \"Bert.Recall\": [],\n",
    "    \"Bert.F1\": [],\n",
    "    \"BLEU\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ground truths\n",
    "generated_answer_true = [generated_data[\"test\"][\"answers\"][idx][\"text\"][0] for idx in range(generated_data[\"test\"].shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = None\n",
    "pipe = None\n",
    "\n",
    "for model in models:\n",
    "    model_data = model.split(\"-\")\n",
    "    year = model_data[-2]\n",
    "\n",
    "    if int(year) != YEAR:\n",
    "        continue\n",
    "\n",
    "    print(f\"Evaluating model {model}...\")\n",
    "    model_name = \"-\".join(model_data[:2])\n",
    "    data_name = model_data[-1]\n",
    "\n",
    "    generator = Seq2SeqGenerator(model_name_or_path=f\"{MODEL_PATH}/{model}\", input_converter=_BartEli5Converter(), use_gpu=True)\n",
    "    pipe = Pipeline()\n",
    "    pipe.add_node(component=retriever, name=\"Retriever\", inputs=[\"Query\"])\n",
    "    pipe.add_node(component=generator, name=\"Model\", inputs=[\"Retriever\"])\n",
    "\n",
    "    results[\"Model\"].append(model_name)\n",
    "    results[\"DPR-ft\"].append(False)\n",
    "    results[\"Data\"].append(data_name)\n",
    "\n",
    "    answer_pred = [inference_answer(pipe, generated_data[\"test\"][\"question\"][idx]) for idx in range(generated_data[\"test\"].shape[0])]\n",
    "\n",
    "    results_1 = bertscore.compute(predictions=answer_pred, references=generated_answer_true, lang=\"en\")\n",
    "    results_2 = bleu.compute(predictions=answer_pred, references=generated_answer_true)\n",
    "\n",
    "    results[\"Bert.Precision\"].append(np.array(results_1[\"precision\"]).mean())\n",
    "    results[\"Bert.Recall\"].append(np.array(results_1[\"recall\"]).mean())\n",
    "    results[\"Bert.F1\"].append(np.array(results_1[\"f1\"]).mean())\n",
    "    results[\"BLEU\"].append(results_2[\"bleu\"])\n",
    "\n",
    "    del generator\n",
    "    del pipe\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear cache\n",
    "del retriever\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Load the fine-tuned retriever\n",
    "retriever = DensePassageRetriever.load(load_dir=f\"../../models/DPR/{YEAR}\", document_store=document_store, use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = None\n",
    "pipe = None\n",
    "\n",
    "for model in models:\n",
    "    model_data = model.split(\"-\")\n",
    "    year = model_data[-2]\n",
    "\n",
    "    if int(year) != YEAR:\n",
    "        continue\n",
    "\n",
    "    print(f\"Evaluating model {model}...\")\n",
    "    model_name = \"-\".join(model_data[:2])\n",
    "    data_name = model_data[-1]\n",
    "\n",
    "    generator = Seq2SeqGenerator(model_name_or_path=f\"{MODEL_PATH}/{model}\", input_converter=_BartEli5Converter(), use_gpu=True)\n",
    "    pipe = Pipeline()\n",
    "    pipe.add_node(component=retriever, name=\"Retriever\", inputs=[\"Query\"])\n",
    "    pipe.add_node(component=generator, name=\"Model\", inputs=[\"Retriever\"])\n",
    "\n",
    "    results[\"Model\"].append(model_name)\n",
    "    results[\"DPR-ft\"].append(True)\n",
    "    results[\"Data\"].append(data_name)\n",
    "\n",
    "    answer_pred = [inference_answer(pipe, generated_data[\"test\"][\"question\"][idx]) for idx in range(generated_data[\"test\"].shape[0])]\n",
    "\n",
    "    results_1 = bertscore.compute(predictions=answer_pred, references=generated_answer_true, lang=\"en\")\n",
    "    results_2 = bleu.compute(predictions=answer_pred, references=generated_answer_true)\n",
    "\n",
    "    results[\"Bert.Precision\"].append(np.array(results_1[\"precision\"]).mean())\n",
    "    results[\"Bert.Recall\"].append(np.array(results_1[\"recall\"]).mean())\n",
    "    results[\"Bert.F1\"].append(np.array(results_1[\"f1\"]).mean())\n",
    "    results[\"BLEU\"].append(results_2[\"bleu\"])\n",
    "\n",
    "    del generator\n",
    "    del pipe\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    display(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_ds_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

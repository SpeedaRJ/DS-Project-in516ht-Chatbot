{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from datasets import *\n",
    "import numpy as np\n",
    "import pathlib as pl\n",
    "import pandas as pd\n",
    "import torch\n",
    "import ast\n",
    "import os\n",
    "from haystack.document_stores import FAISSDocumentStore\n",
    "from haystack.nodes import DensePassageRetriever, TransformersReader\n",
    "from haystack.pipelines import Pipeline\n",
    "\n",
    "YEAR = 2020\n",
    "\n",
    "MODEL_PATH = \"../../data/models/BERT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_data = load_dataset('csv', data_files=f\"../../data/clean/sustainability-report-{YEAR}-squad-format.csv\",\n",
    "                    delimiter=\";\", split='train').train_test_split(test_size=0.3, shuffle=True, seed=42)\n",
    "\n",
    "generated_data[\"test\"] = generated_data[\"test\"].map(\n",
    "    lambda example: ast.literal_eval(example[\"answers\"]))\n",
    "generated_data[\"test\"] = generated_data[\"test\"].map(lambda example: {\"question\": example[\"question\"], \"context\": example[\"context\"], \"answers\": {\n",
    "                                  \"text\": example[\"text\"], \"answer_start\": example[\"answer_start\"]}})\n",
    "generated_data[\"test\"].remove_columns([\"text\", \"answer_start\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertscore = evaluate.load(\"bertscore\")\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "squad_v2 = evaluate.load(\"squad_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_store = FAISSDocumentStore.load(index_path=\"document_store.faiss\", config_path=\"document_store.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rjutr\\miniconda3\\envs\\project_ds_2\\lib\\site-packages\\torch\\_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "retriever = DensePassageRetriever(\n",
    "    document_store=document_store,\n",
    "    query_embedding_model=\"facebook/dpr-question_encoder-single-nq-base\",\n",
    "    passage_embedding_model=\"facebook/dpr-ctx_encoder-single-nq-base\",\n",
    "    use_gpu=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device\n",
    "device = torch.device(\n",
    "    \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Define the prediction function\n",
    "def inference_answer(model, question):\n",
    "    question = question\n",
    "    with torch.no_grad():\n",
    "        outputs = model.run(query=question, params={\"Model\": {\"top_k\": 3}})\n",
    "    return outputs[\"answers\"][0].answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = os.listdir(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    \"Model\": [],\n",
    "    \"Data\": [],\n",
    "    \"Bert.Precision\": [],\n",
    "    \"Bert.Recall\": [],\n",
    "    \"Bert.F1\": [],\n",
    "    \"BLEU\": [],\n",
    "    \"Squad.Exact\": [],\n",
    "    \"Squad.F1\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_answer_true = [generated_data[\"test\"][\"answers\"][idx][\"text\"][0] for idx in range(generated_data[\"test\"].shape[0])]\n",
    "squad_references = [{\"answers\": {\"answer_start\": [answer[\"answer_start\"][0]], \"text\": [\n",
    "    answer[\"text\"][0]]}, \"id\": str(id)} for id, answer in zip(generated_data[\"test\"][\"id\"], generated_data[\"test\"][\"answers\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = None\n",
    "pipe = None\n",
    "\n",
    "for model in models:\n",
    "    model_data = model.split(\"-\")\n",
    "    year = model_data[-2]\n",
    "\n",
    "    if int(year) != YEAR:\n",
    "        continue\n",
    "\n",
    "    print(f\"Evaluating model {model}...\")\n",
    "    model_name = \"-\".join(model_data[:2])\n",
    "    data_name = model_data[-1]\n",
    "\n",
    "    reader = TransformersReader(model_name_or_path=f\"{MODEL_PATH}/{model}\", use_gpu=True)\n",
    "    pipe = Pipeline()\n",
    "    pipe.add_node(component=retriever, name=\"Retriever\", inputs=[\"Query\"])\n",
    "    pipe.add_node(component=reader, name=\"Model\", inputs=[\"Retriever\"])\n",
    "\n",
    "    results[\"Model\"].append(model_name)\n",
    "    results[\"Data\"].append(data_name)\n",
    "\n",
    "    answer_pred = [inference_answer(pipe, generated_data[\"test\"][\"question\"][idx]) for idx in range(generated_data[\"test\"].shape[0])]\n",
    "    squad_predictions = [{\"id\": str(id), \"prediction_text\": answer, \"no_answer_probability\": 0.}\n",
    "               for id, answer in zip(generated_data[\"test\"][\"id\"], answer_pred)]\n",
    "\n",
    "    results_1 = bertscore.compute(predictions=answer_pred, references=generated_answer_true, lang=\"en\")\n",
    "    results_2 = bleu.compute(predictions=answer_pred, references=generated_answer_true)\n",
    "    results_3 = squad_v2.compute(predictions=squad_predictions, references=squad_references)\n",
    "\n",
    "    results[\"Bert.Precision\"].append(np.array(results_1[\"precision\"]).mean())\n",
    "    results[\"Bert.Recall\"].append(np.array(results_1[\"recall\"]).mean())\n",
    "    results[\"Bert.F1\"].append(np.array(results_1[\"f1\"]).mean())\n",
    "    results[\"BLEU\"].append(results_2[\"bleu\"])\n",
    "    results[\"Squad.Exact\"].append(results_3[\"exact\"])\n",
    "    results[\"Squad.F1\"].append(results_3[\"f1\"])\n",
    "\n",
    "    del reader\n",
    "    del pipe\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_ds_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

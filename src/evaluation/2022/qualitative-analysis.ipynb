{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import evaluate\n",
    "from datasets import *\n",
    "import numpy as np\n",
    "import pathlib as pl\n",
    "import pandas as pd\n",
    "import torch\n",
    "import ast\n",
    "import os\n",
    "from haystack.document_stores import FAISSDocumentStore\n",
    "from haystack.nodes import DensePassageRetriever, TransformersReader, Seq2SeqGenerator\n",
    "from haystack.nodes.answer_generator.transformers import _BartEli5Converter\n",
    "from haystack.pipelines import Pipeline\n",
    "\n",
    "# Year identifier\n",
    "YEAR = 2022\n",
    "\n",
    "# Standard model path\n",
    "MODEL_PATH_B = \"../../models/BERT\"\n",
    "MODEL_PATH_T = \"../../models/T5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "written_data = load_dataset('csv', data_files=f\"../../data/clean/squad/QA_SR_2022_Expert-squad-format.csv\",\n",
    "                        delimiter=\";\", split='train').train_test_split(test_size=0.3, shuffle=True, seed=42)\n",
    "# Reformat the train and test set such as they adhere to the SQuAD format (reading from cvs loads strings not objects as expected)\n",
    "written_data[\"test\"] = written_data[\"test\"].map(\n",
    "    lambda example: ast.literal_eval(example[\"answers\"]))\n",
    "# Replace all \"\\n\" with \" \" in the context, answers and questions\n",
    "written_data[\"test\"] = written_data[\"test\"].map(lambda example: {\"question\": example[\"question\"], \"context\": example[\"context\"], \"answers\": {\n",
    "                                  \"text\": example[\"text\"], \"answer_start\": example[\"answer_start\"]}})\n",
    "written_data[\"test\"] = written_data[\"test\"].map(lambda example: {\"question\": example[\"question\"].replace(\"\\n\", \" \"), \"context\": example[\"context\"].replace(\"\\n\", \" \"), \"answers\": {\n",
    "                                \"text\": [example[\"answers\"][\"text\"][0].replace(\"\\n\", \" \")], \"answer_start\": example[\"answers\"][\"answer_start\"]}})\n",
    "written_data[\"test\"].remove_columns([\"text\", \"answer_start\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load document Store\n",
    "document_store = FAISSDocumentStore.load(index_path=\"document_store.faiss\", config_path=\"document_store.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned retriever\n",
    "retriever = DensePassageRetriever.load(load_dir=f\"../../models/DPR/{YEAR}\", document_store=document_store, use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device\n",
    "device = torch.device(\n",
    "    \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Define the prediction function\n",
    "def inference_answer(model, question):\n",
    "    question = question\n",
    "    with torch.no_grad():\n",
    "        outputs = model.run(query=question, params={\"Model\": {\"top_k\": 1}, \"Retriever\": {\"top_k\": 1}})\n",
    "    return outputs[\"answers\"][0].answer, outputs[\"documents\"][0].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boilerplate results\n",
    "results = {\n",
    "    \"Model\": [],\n",
    "    \"Question\": [],\n",
    "    \"Ground Truth Context\": [],\n",
    "    \"Ground Truth Answer\": [],\n",
    "    \"Retrieved Context\": [],\n",
    "    \"Extracted/Generated Answer\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = TransformersReader(model_name_or_path=f\"{MODEL_PATH_B}/distilbert-base-cased-distilled-squad-finetuned-NLB-QA-2022-handwritten\", use_gpu=True)\n",
    "pipe = Pipeline()\n",
    "pipe.add_node(component=retriever, name=\"Retriever\", inputs=[\"Query\"])\n",
    "pipe.add_node(component=reader, name=\"Model\", inputs=[\"Retriever\"])\n",
    "\n",
    "for idx in range(written_data[\"test\"].shape[0]):\n",
    "    results[\"Model\"].append(\"distilbert-base-handwritten\")\n",
    "    row = written_data[\"test\"][idx]\n",
    "    results[\"Question\"].append(row[\"question\"].strip())\n",
    "    results[\"Ground Truth Context\"].append(row[\"context\"])\n",
    "    results[\"Ground Truth Answer\"].append(row[\"answers\"][\"text\"][0])\n",
    "    gotten_answer, gotten_context = inference_answer(pipe, row[\"question\"].strip())\n",
    "    results[\"Retrieved Context\"].append(gotten_context)\n",
    "    results[\"Extracted/Generated Answer\"].append(gotten_answer)\n",
    "\n",
    "del reader\n",
    "del pipe\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = TransformersReader(model_name_or_path=f\"{MODEL_PATH_B}/distilbert-base-cased-distilled-squad-finetuned-NLB-QA-2042-full_combined\", use_gpu=True)\n",
    "pipe = Pipeline()\n",
    "pipe.add_node(component=retriever, name=\"Retriever\", inputs=[\"Query\"])\n",
    "pipe.add_node(component=reader, name=\"Model\", inputs=[\"Retriever\"])\n",
    "\n",
    "for idx in range(written_data[\"test\"].shape[0]):\n",
    "    results[\"Model\"].append(\"distilbert-base-full_combined\")\n",
    "    row = written_data[\"test\"][idx]\n",
    "    results[\"Question\"].append(row[\"question\"].strip())\n",
    "    results[\"Ground Truth Context\"].append(row[\"context\"])\n",
    "    results[\"Ground Truth Answer\"].append(row[\"answers\"][\"text\"][0])\n",
    "    gotten_answer, gotten_context = inference_answer(pipe, row[\"question\"].strip())\n",
    "    results[\"Retrieved Context\"].append(gotten_context)\n",
    "    results[\"Extracted/Generated Answer\"].append(gotten_answer)\n",
    "\n",
    "del reader\n",
    "del pipe\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = TransformersReader(model_name_or_path=f\"{MODEL_PATH_B}/roberta-base-squad2-finetuned-NLB-QA-2022-handwritten\", use_gpu=True)\n",
    "pipe = Pipeline()\n",
    "pipe.add_node(component=retriever, name=\"Retriever\", inputs=[\"Query\"])\n",
    "pipe.add_node(component=reader, name=\"Model\", inputs=[\"Retriever\"])\n",
    "\n",
    "\n",
    "\n",
    "for idx in range(written_data[\"test\"].shape[0]):\n",
    "    results[\"Model\"].append(\"roberta-base-handwritten\")\n",
    "    row = written_data[\"test\"][idx]\n",
    "    results[\"Question\"].append(row[\"question\"].strip())\n",
    "    results[\"Ground Truth Context\"].append(row[\"context\"])\n",
    "    results[\"Ground Truth Answer\"].append(row[\"answers\"][\"text\"][0])\n",
    "    gotten_answer, gotten_context = inference_answer(pipe, row[\"question\"].strip())\n",
    "    results[\"Retrieved Context\"].append(gotten_context)\n",
    "    results[\"Extracted/Generated Answer\"].append(gotten_answer)\n",
    "\n",
    "del reader\n",
    "del pipe\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = TransformersReader(model_name_or_path=f\"{MODEL_PATH_B}/roberta-base-squad2-finetuned-NLB-QA-2042-full_combined\", use_gpu=True)\n",
    "pipe = Pipeline()\n",
    "pipe.add_node(component=retriever, name=\"Retriever\", inputs=[\"Query\"])\n",
    "pipe.add_node(component=reader, name=\"Model\", inputs=[\"Retriever\"])\n",
    "\n",
    "\n",
    "\n",
    "for idx in range(written_data[\"test\"].shape[0]):\n",
    "    results[\"Model\"].append(\"roberta-base-full_combined\")\n",
    "    row = written_data[\"test\"][idx]\n",
    "    results[\"Question\"].append(row[\"question\"].strip())\n",
    "    results[\"Ground Truth Context\"].append(row[\"context\"])\n",
    "    results[\"Ground Truth Answer\"].append(row[\"answers\"][\"text\"][0])\n",
    "    gotten_answer, gotten_context = inference_answer(pipe, row[\"question\"].strip())\n",
    "    results[\"Retrieved Context\"].append(gotten_context)\n",
    "    results[\"Extracted/Generated Answer\"].append(gotten_answer)\n",
    "\n",
    "del reader\n",
    "del pipe\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Seq2SeqGenerator(model_name_or_path=f\"{MODEL_PATH_T}/t5-base-finetuned-squadv2-finetuned-NLB-QA-2022-handwritten\", input_converter=_BartEli5Converter(), use_gpu=True)\n",
    "pipe = Pipeline()\n",
    "pipe.add_node(component=retriever, name=\"Retriever\", inputs=[\"Query\"])\n",
    "pipe.add_node(component=generator, name=\"Model\", inputs=[\"Retriever\"])\n",
    "\n",
    "\n",
    "\n",
    "for idx in range(written_data[\"test\"].shape[0]):\n",
    "    results[\"Model\"].append(\"t5-base-handwritten\")\n",
    "    row = written_data[\"test\"][idx]\n",
    "    results[\"Question\"].append(row[\"question\"].strip())\n",
    "    results[\"Ground Truth Context\"].append(row[\"context\"])\n",
    "    results[\"Ground Truth Answer\"].append(row[\"answers\"][\"text\"][0])\n",
    "    gotten_answer, gotten_context = inference_answer(pipe, row[\"question\"].strip())\n",
    "    results[\"Retrieved Context\"].append(gotten_context)\n",
    "    results[\"Extracted/Generated Answer\"].append(gotten_answer)\n",
    "\n",
    "del generator\n",
    "del pipe\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Seq2SeqGenerator(model_name_or_path=f\"{MODEL_PATH_T}/t5-small-finetuned-squadv2-finetuned-NLB-QA-2022-handwritten\", input_converter=_BartEli5Converter(), use_gpu=True)\n",
    "pipe = Pipeline()\n",
    "pipe.add_node(component=retriever, name=\"Retriever\", inputs=[\"Query\"])\n",
    "pipe.add_node(component=generator, name=\"Model\", inputs=[\"Retriever\"])\n",
    "\n",
    "\n",
    "\n",
    "for idx in range(written_data[\"test\"].shape[0]):\n",
    "    results[\"Model\"].append(\"t5-small-handwritten\")\n",
    "    row = written_data[\"test\"][idx]\n",
    "    results[\"Question\"].append(row[\"question\"].strip())\n",
    "    results[\"Ground Truth Context\"].append(row[\"context\"])\n",
    "    results[\"Ground Truth Answer\"].append(row[\"answers\"][\"text\"][0])\n",
    "    gotten_answer, gotten_context = inference_answer(pipe, row[\"question\"].strip())\n",
    "    results[\"Retrieved Context\"].append(gotten_context)\n",
    "    results[\"Extracted/Generated Answer\"].append(gotten_answer)\n",
    "\n",
    "del generator\n",
    "del pipe\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Seq2SeqGenerator(model_name_or_path=f\"{MODEL_PATH_T}/t5-base-finetuned-squadv2-finetuned-NLB-QA-2042-full_combined\", input_converter=_BartEli5Converter(), use_gpu=True)\n",
    "pipe = Pipeline()\n",
    "pipe.add_node(component=retriever, name=\"Retriever\", inputs=[\"Query\"])\n",
    "pipe.add_node(component=generator, name=\"Model\", inputs=[\"Retriever\"])\n",
    "\n",
    "\n",
    "\n",
    "for idx in range(written_data[\"test\"].shape[0]):\n",
    "    results[\"Model\"].append(\"t5-base-full_combined\")\n",
    "    row = written_data[\"test\"][idx]\n",
    "    results[\"Question\"].append(row[\"question\"].strip())\n",
    "    results[\"Ground Truth Context\"].append(row[\"context\"])\n",
    "    results[\"Ground Truth Answer\"].append(row[\"answers\"][\"text\"][0])\n",
    "    gotten_answer, gotten_context = inference_answer(pipe, row[\"question\"].strip())\n",
    "    results[\"Retrieved Context\"].append(gotten_context)\n",
    "    results[\"Extracted/Generated Answer\"].append(gotten_answer)\n",
    "\n",
    "del generator\n",
    "del pipe\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Seq2SeqGenerator(model_name_or_path=f\"{MODEL_PATH_T}/t5-small-finetuned-squadv2-finetuned-NLB-QA-2042-full_combined\", input_converter=_BartEli5Converter(), use_gpu=True)\n",
    "pipe = Pipeline()\n",
    "pipe.add_node(component=retriever, name=\"Retriever\", inputs=[\"Query\"])\n",
    "pipe.add_node(component=generator, name=\"Model\", inputs=[\"Retriever\"])\n",
    "\n",
    "\n",
    "\n",
    "for idx in range(written_data[\"test\"].shape[0]):\n",
    "    results[\"Model\"].append(\"t5-small-full_combined\")\n",
    "    row = written_data[\"test\"][idx]\n",
    "    results[\"Question\"].append(row[\"question\"].strip())\n",
    "    results[\"Ground Truth Context\"].append(row[\"context\"])\n",
    "    results[\"Ground Truth Answer\"].append(row[\"answers\"][\"text\"][0])\n",
    "    gotten_answer, gotten_context = inference_answer(pipe, row[\"question\"].strip())\n",
    "    results[\"Retrieved Context\"].append(gotten_context)\n",
    "    results[\"Extracted/Generated Answer\"].append(gotten_answer)\n",
    "\n",
    "del generator\n",
    "del pipe\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset bz model\n",
    "for model in results_df[\"Model\"].unique():\n",
    "    model_df = results_df[results_df[\"Model\"] == model]\n",
    "    model_df.to_csv(f\"../../data/results/2022-handwritten/{model}-outputs.csv\", index=False, sep=\";\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_ds_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

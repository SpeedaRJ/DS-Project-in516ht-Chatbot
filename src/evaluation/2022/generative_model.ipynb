{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from datasets import *\n",
    "import numpy as np\n",
    "import pathlib as pl\n",
    "import pandas as pd\n",
    "import torch\n",
    "import ast\n",
    "import os\n",
    "from haystack.document_stores import FAISSDocumentStore\n",
    "from haystack.nodes import DensePassageRetriever, Seq2SeqGenerator\n",
    "from haystack.nodes.answer_generator.transformers import _BartEli5Converter\n",
    "from haystack.pipelines import Pipeline\n",
    "\n",
    "YEAR = 2022\n",
    "DPR_FINE_TUNE = False\n",
    "\n",
    "MODEL_PATH = \"../../data/models/T5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/rjutr/.cache/huggingface/datasets/csv/default-844d2bcfb25206a8/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "Loading cached split indices for dataset at C:\\Users\\rjutr\\.cache\\huggingface\\datasets\\csv\\default-844d2bcfb25206a8\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-d6ce3d847b779dd2.arrow and C:\\Users\\rjutr\\.cache\\huggingface\\datasets\\csv\\default-844d2bcfb25206a8\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-29c416daaef0860b.arrow\n",
      "Loading cached processed dataset at C:\\Users\\rjutr\\.cache\\huggingface\\datasets\\csv\\default-844d2bcfb25206a8\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-e0aa9102ca5d7eb5.arrow\n",
      "Loading cached processed dataset at C:\\Users\\rjutr\\.cache\\huggingface\\datasets\\csv\\default-844d2bcfb25206a8\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-4ebe342127815145.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['index', 'question', 'context', 'answers', 'id'],\n",
       "    num_rows: 107\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_data = load_dataset('csv', data_files=f\"../../data/clean/sustainability-report-{YEAR}-squad-format.csv\",\n",
    "                    delimiter=\";\", split='train').train_test_split(test_size=0.3, shuffle=True, seed=42)\n",
    "\n",
    "generated_data[\"test\"] = generated_data[\"test\"].map(\n",
    "    lambda example: ast.literal_eval(example[\"answers\"]))\n",
    "generated_data[\"test\"] = generated_data[\"test\"].map(lambda example: {\"question\": example[\"question\"], \"context\": example[\"context\"], \"answers\": {\n",
    "                                  \"text\": example[\"text\"], \"answer_start\": example[\"answer_start\"]}})\n",
    "generated_data[\"test\"].remove_columns([\"text\", \"answer_start\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/rjutr/.cache/huggingface/datasets/csv/default-54c2d14257a19ede/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "Loading cached split indices for dataset at C:\\Users\\rjutr\\.cache\\huggingface\\datasets\\csv\\default-54c2d14257a19ede\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-3ff29462153ffd09.arrow and C:\\Users\\rjutr\\.cache\\huggingface\\datasets\\csv\\default-54c2d14257a19ede\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-8202b31bf7f0cc8f.arrow\n",
      "Loading cached processed dataset at C:\\Users\\rjutr\\.cache\\huggingface\\datasets\\csv\\default-54c2d14257a19ede\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-68763aea809abd97.arrow\n",
      "Loading cached processed dataset at C:\\Users\\rjutr\\.cache\\huggingface\\datasets\\csv\\default-54c2d14257a19ede\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-50d87829045139cf.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'context', 'answers'],\n",
       "    num_rows: 21\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "written_data = load_dataset('csv', data_files=f\"../../data/clean/sr-2022-questions-answers-ALL-squad-format.csv\",\n",
    "                        delimiter=\";\", split='train').train_test_split(test_size=0.3, shuffle=True, seed=42)\n",
    "written_data[\"test\"] = written_data[\"test\"].map(\n",
    "    lambda example: ast.literal_eval(example[\"answers\"]))\n",
    "written_data[\"test\"] = written_data[\"test\"].map(lambda example: {\"question\": example[\"question\"], \"context\": example[\"context\"], \"answers\": {\n",
    "                                  \"text\": example[\"text\"], \"answer_start\": example[\"answer_start\"]}})\n",
    "written_data[\"test\"].remove_columns([\"text\", \"answer_start\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertscore = evaluate.load(\"bertscore\")\n",
    "bleu = evaluate.load(\"bleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_store = FAISSDocumentStore.load(index_path=\"document_store.faiss\", config_path=\"document_store.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rjutr\\miniconda3\\envs\\project_ds_2\\lib\\site-packages\\torch\\_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "if DPR_FINE_TUNE:\n",
    "    retriever = DensePassageRetriever.load(load_dir=f\"../../data/models/DPR/{YEAR}\", document_store=document_store, use_gpu=True)\n",
    "else: \n",
    "    retriever = DensePassageRetriever(\n",
    "        document_store=document_store,\n",
    "        query_embedding_model=\"facebook/dpr-question_encoder-single-nq-base\",\n",
    "        passage_embedding_model=\"facebook/dpr-ctx_encoder-single-nq-base\",\n",
    "        use_gpu=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device\n",
    "device = torch.device(\n",
    "    \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Define the prediction function\n",
    "def inference_answer(model, question):\n",
    "    question = question\n",
    "    with torch.no_grad():\n",
    "        outputs = model.run(query=question, params={\"Model\": {\"top_k\": 3}})\n",
    "    return outputs[\"answers\"][0].answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = os.listdir(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    \"Model\": [],\n",
    "    \"Data\": [],\n",
    "    \"Bert.Precision\": [],\n",
    "    \"Bert.Recall\": [],\n",
    "    \"Bert.F1\": [],\n",
    "    \"BLEU\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_answer_true = [generated_data[\"test\"][\"answers\"][idx][\"text\"][0] for idx in range(generated_data[\"test\"].shape[0])]\n",
    "written_answer_true = [written_data[\"test\"][\"answers\"][idx][\"text\"][0] for idx in range(written_data[\"test\"].shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model t5-base-finetuned-squadv2-finetuned-NLB-QA-2022-full...\n",
      "Evaluating model t5-base-finetuned-squadv2-finetuned-NLB-QA-2022-handwritten...\n",
      "Evaluating model t5-base-finetuned-squadv2-finetuned-NLB-QA-2022-smaller...\n",
      "Evaluating model t5-small-finetuned-squadv2-finetuned-NLB-QA-2022-full...\n",
      "Evaluating model t5-small-finetuned-squadv2-finetuned-NLB-QA-2022-handwritten...\n",
      "Evaluating model t5-small-finetuned-squadv2-finetuned-NLB-QA-2022-smaller...\n"
     ]
    }
   ],
   "source": [
    "generator = None\n",
    "pipe = None\n",
    "\n",
    "for model in models:\n",
    "    model_data = model.split(\"-\")\n",
    "    year = model_data[-2]\n",
    "\n",
    "    if int(year) != YEAR:\n",
    "        continue\n",
    "\n",
    "    print(f\"Evaluating model {model}...\")\n",
    "    model_name = \"-\".join(model_data[:2])\n",
    "    data_name = model_data[-1]\n",
    "\n",
    "    generator = Seq2SeqGenerator(model_name_or_path=f\"{MODEL_PATH}/{model}\", input_converter=_BartEli5Converter())\n",
    "    pipe = Pipeline()\n",
    "    pipe.add_node(component=retriever, name=\"Retriever\", inputs=[\"Query\"])\n",
    "    pipe.add_node(component=generator, name=\"Model\", inputs=[\"Retriever\"])\n",
    "\n",
    "    results[\"Model\"].append(model_name)\n",
    "    results[\"Data\"].append(data_name)\n",
    "\n",
    "    if \"handwritten\" != data_name:\n",
    "        answer_pred = [inference_answer(pipe, generated_data[\"test\"][\"question\"][idx]) for idx in range(generated_data[\"test\"].shape[0])]\n",
    "\n",
    "        results_1 = bertscore.compute(predictions=answer_pred, references=generated_answer_true, lang=\"en\")\n",
    "        results_2 = bleu.compute(predictions=answer_pred, references=generated_answer_true)\n",
    "    else:\n",
    "        answer_pred = [inference_answer(pipe, written_data[\"test\"][\"question\"][idx]) for idx in range(written_data[\"test\"].shape[0])]\n",
    "\n",
    "        results_1 = bertscore.compute(predictions=answer_pred, references=written_answer_true, lang=\"en\")\n",
    "        results_2 = bleu.compute(predictions=answer_pred, references=written_answer_true)\n",
    "\n",
    "    results[\"Bert.Precision\"].append(np.array(results_1[\"precision\"]).mean())\n",
    "    results[\"Bert.Recall\"].append(np.array(results_1[\"recall\"]).mean())\n",
    "    results[\"Bert.F1\"].append(np.array(results_1[\"f1\"]).mean())\n",
    "    results[\"BLEU\"].append(results_2[\"bleu\"])\n",
    "\n",
    "    del generator\n",
    "    del pipe\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Data</th>\n",
       "      <th>Bert.Precision</th>\n",
       "      <th>Bert.Recall</th>\n",
       "      <th>Bert.F1</th>\n",
       "      <th>BLEU</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t5-base</td>\n",
       "      <td>full</td>\n",
       "      <td>0.918457</td>\n",
       "      <td>0.921503</td>\n",
       "      <td>0.919821</td>\n",
       "      <td>0.268291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t5-base</td>\n",
       "      <td>handwritten</td>\n",
       "      <td>0.855109</td>\n",
       "      <td>0.866702</td>\n",
       "      <td>0.859748</td>\n",
       "      <td>0.213467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t5-base</td>\n",
       "      <td>smaller</td>\n",
       "      <td>0.916145</td>\n",
       "      <td>0.917903</td>\n",
       "      <td>0.916849</td>\n",
       "      <td>0.247052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t5-small</td>\n",
       "      <td>full</td>\n",
       "      <td>0.922533</td>\n",
       "      <td>0.922373</td>\n",
       "      <td>0.922294</td>\n",
       "      <td>0.198704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t5-small</td>\n",
       "      <td>handwritten</td>\n",
       "      <td>0.881346</td>\n",
       "      <td>0.862474</td>\n",
       "      <td>0.871060</td>\n",
       "      <td>0.084763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>t5-small</td>\n",
       "      <td>smaller</td>\n",
       "      <td>0.915237</td>\n",
       "      <td>0.914667</td>\n",
       "      <td>0.914778</td>\n",
       "      <td>0.261799</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Model         Data  Bert.Precision  Bert.Recall   Bert.F1      BLEU\n",
       "0   t5-base         full        0.918457     0.921503  0.919821  0.268291\n",
       "1   t5-base  handwritten        0.855109     0.866702  0.859748  0.213467\n",
       "2   t5-base      smaller        0.916145     0.917903  0.916849  0.247052\n",
       "3  t5-small         full        0.922533     0.922373  0.922294  0.198704\n",
       "4  t5-small  handwritten        0.881346     0.862474  0.871060  0.084763\n",
       "5  t5-small      smaller        0.915237     0.914667  0.914778  0.261799"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_ds_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

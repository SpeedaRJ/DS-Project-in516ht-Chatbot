{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer imports\n",
    "from transformers import DistilBertTokenizerFast\n",
    "\n",
    "# Data handling imports\n",
    "from datasets import *\n",
    "import numpy as np\n",
    "\n",
    "# General imports\n",
    "import torch\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/rjutr/.cache/huggingface/datasets/csv/default-656364e83e74f69c/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset from file and split it into train and test datasets\n",
    "data = load_dataset('csv', data_files=\"../data/clean/sustainability-report-2020-squad-format.csv\", delimiter=\";\", split='train').train_test_split(test_size=0.3, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d07204c536743a5a885ca69fadcb011",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/56 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d92f651736040a1968c539bb233abf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/56 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bd51aa4ad0a41c6955c8b02cc3d536d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/129 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8337fa7f7a74eba90ac6486bcc0b331",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/129 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'context', 'answers', 'id'],\n",
       "    num_rows: 129\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reformat the train and test set such as they adhere to the SQuAD format (reading from cvs loads strings not objects as expected)\n",
    "data[\"test\"] = data[\"test\"].map(lambda example: ast.literal_eval(example[\"answers\"]))\n",
    "data[\"test\"] = data[\"test\"].map(lambda example: {\"question\": example[\"question\"], \"context\": example[\"context\"], \"answers\": {\"text\": example[\"text\"], \"answer_start\": example[\"answer_start\"]}})\n",
    "data[\"test\"].remove_columns([\"text\", \"answer_start\"])\n",
    "\n",
    "data[\"train\"] = data[\"train\"].map(lambda example: ast.literal_eval(example[\"answers\"]))\n",
    "data[\"train\"] = data[\"train\"].map(lambda example: {\"question\": example[\"question\"], \"context\": example[\"context\"], \"answers\": {\"text\": example[\"text\"], \"answer_start\": example[\"answer_start\"]}})\n",
    "data[\"train\"].remove_columns([\"text\", \"answer_start\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-cased-distilled-squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b31ef54e5304197a9f61f25ed8c06d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/129 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e3ff65f541c441c8ceb962f60b100fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/56 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_sample_data(data):\n",
    "  # Tokenize the data\n",
    "  tokenized_feature = tokenizer(\n",
    "    data[\"question\"],\n",
    "    data[\"context\"],\n",
    "    max_length = 384,\n",
    "    return_overflowing_tokens=True,\n",
    "    stride=128,\n",
    "    truncation=\"only_second\",\n",
    "    padding = \"max_length\",\n",
    "    return_offsets_mapping=True,\n",
    "  )\n",
    "\n",
    "  # When it overflows, multiple rows will be returned for a single example.\n",
    "  # The following then gets the array of corresponding the original sample index.\n",
    "  sample_mapping = tokenized_feature.pop(\"overflow_to_sample_mapping\")\n",
    "  # Get the array of [start_char, end_char + 1] in each token.\n",
    "  # The shape is [returned_row_size, max_length]\n",
    "  offset_mapping = tokenized_feature.pop(\"offset_mapping\")\n",
    "\n",
    "  start_positions = []\n",
    "  end_positions = []\n",
    "  for i, offset in enumerate(offset_mapping):\n",
    "    sample_index = sample_mapping[i]\n",
    "    answers = data[\"answers\"][sample_index]\n",
    "    start_char = answers[\"answer_start\"][0]\n",
    "    end_char = start_char + len(answers[\"text\"][0]) - 1\n",
    "    # The format of sequence_ids is [None, 0, ..., 0, None, None, 1, ..., 1, None, None, ...]\n",
    "    # in which question's token is 0 and contex's token is 1\n",
    "    sequence_ids = tokenized_feature.sequence_ids(i)\n",
    "    # find the start and end index of context\n",
    "    idx = 0\n",
    "    while sequence_ids[idx] != 1:\n",
    "      idx += 1\n",
    "    context_start = idx\n",
    "    while sequence_ids[idx] == 1:\n",
    "      idx += 1\n",
    "    context_end = idx - 1\n",
    "    # Set start positions and end positions in inputs_ids\n",
    "    # Note: The second element in offset is end_char + 1\n",
    "    # if offset[context_start][0] > end_char or offset[context_end][1] <= start_char:\n",
    "    if not (offset[context_start][0] <= start_char and end_char < offset[context_end][1]):\n",
    "      # The case that answer is not inside the context\n",
    "      ## Note : Some tokenizer (such as, tokenizer in rinna model) doesn't place CLS\n",
    "      ## for the first token in sequence, and I then set -1 as positions.\n",
    "      ## (Later I'll process rows with start_positions=-1.)\n",
    "      start_positions.append(-1)\n",
    "      end_positions.append(-1)\n",
    "    else:\n",
    "      # The case that answer is found in the context\n",
    "\n",
    "      # Set start position\n",
    "      idx = context_start\n",
    "      while offset[idx][0] < start_char:\n",
    "        idx += 1\n",
    "      if offset[idx][0] == start_char:\n",
    "        start_positions.append(idx)\n",
    "      else:\n",
    "        start_positions.append(idx - 1)\n",
    "\n",
    "      # Set end position\n",
    "      idx = context_end\n",
    "      while offset[idx][1] > end_char + 1:\n",
    "        idx -= 1\n",
    "      if offset[idx][1] == end_char + 1:\n",
    "        end_positions.append(idx)\n",
    "      else:\n",
    "        end_positions.append(idx + 1)\n",
    "\n",
    "  # Build result\n",
    "  tokenized_feature[\"start_positions\"] = start_positions\n",
    "  tokenized_feature[\"end_positions\"] = end_positions   \n",
    "  return tokenized_feature\n",
    "\n",
    "# Run conversion\n",
    "tokenized_ds = data.map(\n",
    "  tokenize_sample_data,\n",
    "  remove_columns=[\"id\", \"context\", \"question\", \"answers\"],\n",
    "  batched=True,\n",
    "  batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd0be69a600a40838f0c5b4bae201ca2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/129 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7105db9520294cc6b1ced6c8c3d591e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/56 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Remove rows with no answer\n",
    "tokenized_ds = tokenized_ds.filter(lambda x: x[\"start_positions\"] != -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QA model import\n",
    "from transformers import AutoModelForQuestionAnswering\n",
    "\n",
    "# Set the device\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-cased-distilled-squad\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer import\n",
    "from transformers import DefaultDataCollator\n",
    "\n",
    "# Set the data collator\n",
    "data_collator = DefaultDataCollator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer import\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "  output_dir = \"distilbert-nlb-qa\",\n",
    "  log_level = \"error\",\n",
    "  num_train_epochs = 10,\n",
    "  learning_rate = 2e-5,\n",
    "  lr_scheduler_type = \"linear\",\n",
    "  warmup_steps = 2,\n",
    "  per_device_train_batch_size = 16,\n",
    "  per_device_eval_batch_size = 16,\n",
    "  gradient_accumulation_steps = 16,\n",
    "  evaluation_strategy = \"steps\",\n",
    "  eval_steps = 2,\n",
    "  save_steps = 2,\n",
    "  logging_steps = 2,\n",
    "  push_to_hub = False\n",
    ")\n",
    "\n",
    "# Define the trainer\n",
    "trainer = Trainer(\n",
    "  model = model,\n",
    "  args = training_args,\n",
    "  data_collator = data_collator,\n",
    "  train_dataset = tokenized_ds[\"train\"],\n",
    "  eval_dataset = tokenized_ds[\"test\"],\n",
    "  tokenizer = tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rjutr\\miniconda3\\envs\\project_ds\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faf8f144e1554209803f0fcc3b8cfbea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3398, 'learning_rate': 2e-05, 'epoch': 1.78}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d98e077ab3994de3a64678db41c64475",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9894131422042847, 'eval_runtime': 0.275, 'eval_samples_per_second': 203.636, 'eval_steps_per_second': 14.545, 'epoch': 1.78}\n",
      "{'loss': 0.1941, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e88694f2a1b649aab18e2169c47cae2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9486815333366394, 'eval_runtime': 0.2603, 'eval_samples_per_second': 215.175, 'eval_steps_per_second': 15.37, 'epoch': 3.0}\n",
      "{'loss': 0.1212, 'learning_rate': 1e-05, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "113e0a71ab4f46ba8e271de7fe2f5e6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9189756512641907, 'eval_runtime': 0.259, 'eval_samples_per_second': 216.217, 'eval_steps_per_second': 15.444, 'epoch': 4.0}\n",
      "{'loss': 0.2316, 'learning_rate': 5e-06, 'epoch': 5.33}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27d37dcde97a4cdcad5b54ff520640c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9028012156486511, 'eval_runtime': 0.259, 'eval_samples_per_second': 216.218, 'eval_steps_per_second': 15.444, 'epoch': 5.33}\n",
      "{'loss': 0.1393, 'learning_rate': 0.0, 'epoch': 7.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98e29d2629064caba6d29fc3c9f83d7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8972963094711304, 'eval_runtime': 0.273, 'eval_samples_per_second': 205.128, 'eval_steps_per_second': 14.652, 'epoch': 7.0}\n",
      "{'train_runtime': 28.4893, 'train_samples_per_second': 45.28, 'train_steps_per_second': 0.351, 'train_loss': 0.2051896035671234, 'epoch': 7.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10, training_loss=0.2051896035671234, metrics={'train_runtime': 28.4893, 'train_samples_per_second': 45.28, 'train_steps_per_second': 0.351, 'train_loss': 0.2051896035671234, 'epoch': 7.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "955857379a834a06a7b44295fc7af267",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.8972963094711304,\n",
       " 'eval_runtime': 0.457,\n",
       " 'eval_samples_per_second': 122.543,\n",
       " 'eval_steps_per_second': 8.753,\n",
       " 'epoch': 7.0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prediction function\n",
    "def inference_answer(model, question, context):\n",
    "  question = question\n",
    "  context = context\n",
    "  test_feature = tokenizer(\n",
    "    question,\n",
    "    context,\n",
    "    max_length=318\n",
    "  )\n",
    "  with torch.no_grad():\n",
    "    outputs = model(torch.tensor([test_feature[\"input_ids\"]]).to(device))\n",
    "  start_logits = outputs.start_logits.cpu().numpy()\n",
    "  end_logits = outputs.end_logits.cpu().numpy()\n",
    "  answer_ids = test_feature[\"input_ids\"][np.argmax(start_logits):np.argmax(end_logits)+1]\n",
    "  return \" \".join(tokenizer.batch_decode(answer_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an array of predictions and an array of true answers\n",
    "answer_pred = [inference_answer(model, data[\"test\"][\"question\"][idx], data[\"test\"][\"context\"][idx]) for idx in range(data[\"test\"].shape[0])]\n",
    "answer_true = [data[\"test\"][\"answers\"][idx][\"text\"][0] for idx in range(data[\"test\"].shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.9058806672692299, Precision: 0.8877458412732396, Recall: 0.9261775421244758\n"
     ]
    }
   ],
   "source": [
    "# Importing the evaluation library\n",
    "import evaluate\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "results = bertscore.compute(predictions=answer_pred, references=answer_true, lang=\"en\")\n",
    "# Embeddings bases evaluation\n",
    "print(f\"F1: {np.array(results['f1']).mean()}, Precision: {np.array(results['precision']).mean()}, Recall: {np.array(results['recall']).mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact': 28.571428571428573,\n",
       " 'f1': 47.18719894957925,\n",
       " 'total': 56,\n",
       " 'HasAns_exact': 28.571428571428573,\n",
       " 'HasAns_f1': 47.18719894957925,\n",
       " 'HasAns_total': 56,\n",
       " 'best_exact': 28.571428571428573,\n",
       " 'best_exact_thresh': 0.0,\n",
       " 'best_f1': 47.18719894957925,\n",
       " 'best_f1_thresh': 0.0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SQuAD v2.0 evaluation\n",
    "squad_v2_metric = evaluate.load(\"squad_v2\")\n",
    "references = [{\"answers\": {\"answer_start\": [answer[\"answer_start\"][0]], \"text\": [answer[\"text\"][0]]}, \"id\": str(id)} for id, answer in zip(data[\"test\"][\"id\"], data[\"test\"][\"answers\"])]\n",
    "predictions = [{\"id\": str(id), \"prediction_text\": answer, \"no_answer_probability\": 0.} for id, answer in zip(data[\"test\"][\"id\"], answer_pred)]\n",
    "results = squad_v2_metric.compute(predictions=predictions, references=references)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.13916222050111174,\n",
       " 'precisions': [0.24453280318091453,\n",
       "  0.16331096196868009,\n",
       "  0.11528822055137844,\n",
       "  0.08146067415730338],\n",
       " 'brevity_penalty': 1.0,\n",
       " 'length_ratio': 2.54040404040404,\n",
       " 'translation_length': 503,\n",
       " 'reference_length': 198}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu = evaluate.load(\"bleu\")\n",
    "references = [[answer] for answer in answer_true]\n",
    "predictions = answer_pred\n",
    "# N-Gram based evaluation\n",
    "results = bleu.compute(predictions=predictions, references=references)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the same model without fine-tuning on our dataset\n",
    "model_no_ft = AutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-cased-distilled-squad\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate another set of predictions\n",
    "answer_pred_no_ft = [inference_answer(model_no_ft, data[\"test\"][\"question\"][idx], data[\"test\"][\"context\"][idx]) for idx in range(data[\"test\"].shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.9056908296687263, Precision: 0.8875610434583255, Recall: 0.9259395631296294\n"
     ]
    }
   ],
   "source": [
    "results = bertscore.compute(predictions=answer_pred_no_ft, references=answer_true, lang=\"en\")\n",
    "# Embeddings bases evaluation\n",
    "print(f\"F1: {np.array(results['f1']).mean()}, Precision: {np.array(results['precision']).mean()}, Recall: {np.array(results['recall']).mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact': 28.571428571428573,\n",
       " 'f1': 46.38297759548578,\n",
       " 'total': 56,\n",
       " 'HasAns_exact': 28.571428571428573,\n",
       " 'HasAns_f1': 46.38297759548578,\n",
       " 'HasAns_total': 56,\n",
       " 'best_exact': 28.571428571428573,\n",
       " 'best_exact_thresh': 0.0,\n",
       " 'best_f1': 46.38297759548578,\n",
       " 'best_f1_thresh': 0.0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "references = [{\"answers\": {\"answer_start\": [answer[\"answer_start\"][0]], \"text\": [answer[\"text\"][0]]}, \"id\": str(id)} for id, answer in zip(data[\"test\"][\"id\"], data[\"test\"][\"answers\"])]\n",
    "predictions = [{\"id\": str(id), \"prediction_text\": answer, \"no_answer_probability\": 0.} for id, answer in zip(data[\"test\"][\"id\"], answer_pred_no_ft)]\n",
    "# SQuAD v2.0 evaluation\n",
    "results = squad_v2_metric.compute(predictions=predictions, references=references)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.13037756203419767,\n",
       " 'precisions': [0.23625254582484725,\n",
       "  0.15402298850574714,\n",
       "  0.1056701030927835,\n",
       "  0.07514450867052024],\n",
       " 'brevity_penalty': 1.0,\n",
       " 'length_ratio': 2.4797979797979797,\n",
       " 'translation_length': 491,\n",
       " 'reference_length': 198}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "references = [[answer] for answer in answer_true]\n",
    "predictions = answer_pred_no_ft\n",
    "# N-Gram based evaluation\n",
    "results = bleu.compute(predictions=predictions, references=references)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locally save the model\n",
    "trainer.save_model(\"./distilbert-nlb-qa\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

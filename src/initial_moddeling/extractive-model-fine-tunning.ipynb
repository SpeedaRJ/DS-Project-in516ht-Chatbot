{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "from datasets import *\n",
    "import torch\n",
    "import ast\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to C:/Users/rjutr/.cache/huggingface/datasets/csv/default-6a9a3e730f68f403/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20a89b4df91a4c3e963bc5564d1aca5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20e14f0b24b9429a9644331f820ebbe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d60306991d443e1855bfe7bb5f6f25f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to C:/Users/rjutr/.cache/huggingface/datasets/csv/default-6a9a3e730f68f403/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "data = load_dataset('csv', data_files=\"../data/clean/sustainability-report-2020-squad-format.csv\", delimiter=\";\", split='train').train_test_split(test_size=0.3, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e9349a0639143a9b9321f977bb05c78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/56 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe520e89353846e59a5378762048016b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/56 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e7dce9d240c429f87b2dc6d92185a5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/129 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7b4a338cff2430497ea4ec14b30dbb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/129 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'context', 'answers', 'id'],\n",
       "    num_rows: 129\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"test\"] = data[\"test\"].map(lambda example: ast.literal_eval(example[\"answers\"]))\n",
    "data[\"test\"] = data[\"test\"].map(lambda example: {\"question\": example[\"question\"], \"context\": example[\"context\"], \"answers\": {\"text\": example[\"text\"], \"answer_start\": example[\"answer_start\"]}})\n",
    "data[\"test\"].remove_columns([\"text\", \"answer_start\"])\n",
    "\n",
    "data[\"train\"] = data[\"train\"].map(lambda example: ast.literal_eval(example[\"answers\"]))\n",
    "data[\"train\"] = data[\"train\"].map(lambda example: {\"question\": example[\"question\"], \"context\": example[\"context\"], \"answers\": {\"text\": example[\"text\"], \"answer_start\": example[\"answer_start\"]}})\n",
    "data[\"train\"].remove_columns([\"text\", \"answer_start\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dd4a705de704395a7aca84864fa22d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/129 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73f98a364fed443383522dbf92d9c222",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/56 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_sample_data(data):\n",
    "  # Tokenize\n",
    "  tokenized_feature = tokenizer(\n",
    "    data[\"question\"],\n",
    "    data[\"context\"],\n",
    "    max_length = 384,\n",
    "    return_overflowing_tokens=True,\n",
    "    stride=128,\n",
    "    truncation=\"only_second\",\n",
    "    padding = \"max_length\",\n",
    "    return_offsets_mapping=True,\n",
    "  )\n",
    "\n",
    "  # When it overflows, multiple rows will be returned for a single example.\n",
    "  # The following then gets the array of corresponding the original sample index.\n",
    "  sample_mapping = tokenized_feature.pop(\"overflow_to_sample_mapping\")\n",
    "  # Get the array of [start_char, end_char + 1] in each token.\n",
    "  # The shape is [returned_row_size, max_length]\n",
    "  offset_mapping = tokenized_feature.pop(\"offset_mapping\")\n",
    "\n",
    "  start_positions = []\n",
    "  end_positions = []\n",
    "  for i, offset in enumerate(offset_mapping):\n",
    "    sample_index = sample_mapping[i]\n",
    "    answers = data[\"answers\"][sample_index]\n",
    "    start_char = answers[\"answer_start\"][0]\n",
    "    end_char = start_char + len(answers[\"text\"][0]) - 1\n",
    "    # The format of sequence_ids is [None, 0, ..., 0, None, None, 1, ..., 1, None, None, ...]\n",
    "    # in which question's token is 0 and contex's token is 1\n",
    "    sequence_ids = tokenized_feature.sequence_ids(i)\n",
    "    # find the start and end index of context\n",
    "    idx = 0\n",
    "    while sequence_ids[idx] != 1:\n",
    "      idx += 1\n",
    "    context_start = idx\n",
    "    while sequence_ids[idx] == 1:\n",
    "      idx += 1\n",
    "    context_end = idx - 1\n",
    "    # Set start positions and end positions in inputs_ids\n",
    "    # Note: The second element in offset is end_char + 1\n",
    "    # if offset[context_start][0] > end_char or offset[context_end][1] <= start_char:\n",
    "    if not (offset[context_start][0] <= start_char and end_char < offset[context_end][1]):\n",
    "      # The case that answer is not inside the context\n",
    "      ## Note : Some tokenizer (such as, tokenizer in rinna model) doesn't place CLS\n",
    "      ## for the first token in sequence, and I then set -1 as positions.\n",
    "      ## (Later I'll process rows with start_positions=-1.)\n",
    "      start_positions.append(-1)\n",
    "      end_positions.append(-1)\n",
    "    else:\n",
    "      # The case that answer is found in the context\n",
    "\n",
    "      # Set start position\n",
    "      idx = context_start\n",
    "      while offset[idx][0] < start_char:\n",
    "        idx += 1\n",
    "      if offset[idx][0] == start_char:\n",
    "        start_positions.append(idx)\n",
    "      else:\n",
    "        start_positions.append(idx - 1)\n",
    "\n",
    "      # Set end position\n",
    "      idx = context_end\n",
    "      while offset[idx][1] > end_char + 1:\n",
    "        idx -= 1\n",
    "      if offset[idx][1] == end_char + 1:\n",
    "        end_positions.append(idx)\n",
    "      else:\n",
    "        end_positions.append(idx + 1)\n",
    "\n",
    "  # Build result\n",
    "  tokenized_feature[\"start_positions\"] = start_positions\n",
    "  tokenized_feature[\"end_positions\"] = end_positions   \n",
    "  return tokenized_feature\n",
    "\n",
    "# Run conversion\n",
    "tokenized_ds = data.map(\n",
    "  tokenize_sample_data,\n",
    "  remove_columns=[\"id\", \"context\", \"question\", \"answers\"],\n",
    "  batched=True,\n",
    "  batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f11dfebc6eb4221ae780be29ebc16a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/129 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fce141bb9ee14193ad4ca65df152985c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/56 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_ds = tokenized_ds.filter(lambda x: x[\"start_positions\"] != -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'answer_start', 'input_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
       "        num_rows: 129\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'answer_start', 'input_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
       "        num_rows: 56\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "model = (AutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\").to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DefaultDataCollator\n",
    "\n",
    "data_collator = DefaultDataCollator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir = \"distilbert-nlb-qa\",\n",
    "  log_level = \"error\",\n",
    "  num_train_epochs = 200,\n",
    "  learning_rate = 7e-5,\n",
    "  lr_scheduler_type = \"linear\",\n",
    "  warmup_steps = 100,\n",
    "  per_device_train_batch_size = 16,\n",
    "  per_device_eval_batch_size = 16,\n",
    "  gradient_accumulation_steps = 16,\n",
    "  evaluation_strategy = \"steps\",\n",
    "  eval_steps = 150,\n",
    "  save_steps = 500,\n",
    "  logging_steps = 50,\n",
    "  push_to_hub = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rjutr\\miniconda3\\envs\\project_ds\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35a4db961f714613af7f461eba0e00eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7114, 'learning_rate': 3.5e-05, 'epoch': 33.78}\n",
      "{'loss': 0.0824, 'learning_rate': 7e-05, 'epoch': 67.0}\n",
      "{'loss': 0.0012, 'learning_rate': 3.5e-05, 'epoch': 100.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1133ac25b0ed4e27b509b5c543cf3a0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.4316065311431885, 'eval_runtime': 0.1401, 'eval_samples_per_second': 199.818, 'eval_steps_per_second': 14.273, 'epoch': 100.0}\n",
      "{'loss': 0.0006, 'learning_rate': 0.0, 'epoch': 133.33}\n",
      "{'train_runtime': 247.3689, 'train_samples_per_second': 104.298, 'train_steps_per_second': 0.809, 'train_loss': 0.4489036752842367, 'epoch': 133.33}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=200, training_loss=0.4489036752842367, metrics={'train_runtime': 247.3689, 'train_samples_per_second': 104.298, 'train_steps_per_second': 0.809, 'train_loss': 0.4489036752842367, 'epoch': 133.33})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "  model = model,\n",
    "  args = training_args,\n",
    "  data_collator = data_collator,\n",
    "  train_dataset = tokenized_ds[\"train\"],\n",
    "  eval_dataset = tokenized_ds[\"test\"].select(range(data[\"test\"].shape[0] // 2)),\n",
    "  tokenizer = tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "406f9fdf75e24611abe13006940bfc61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.130403518676758,\n",
       " 'eval_runtime': 0.14,\n",
       " 'eval_samples_per_second': 200.062,\n",
       " 'eval_steps_per_second': 14.29,\n",
       " 'epoch': 133.33}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def inference_answer(model, question, context):\n",
    "  question = question\n",
    "  context = context\n",
    "  test_feature = tokenizer(\n",
    "    question,\n",
    "    context,\n",
    "    max_length=318,\n",
    "  )\n",
    "  with torch.no_grad():\n",
    "    outputs = model(torch.tensor([test_feature[\"input_ids\"]]).to(device))\n",
    "  start_logits = outputs.start_logits.cpu().numpy()\n",
    "  end_logits = outputs.end_logits.cpu().numpy()\n",
    "  answer_ids = test_feature[\"input_ids\"][np.argmax(start_logits):np.argmax(end_logits)+1]\n",
    "  return \" \".join(tokenizer.batch_decode(answer_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_pred = [inference_answer(model, data[\"test\"][\"question\"][idx], data[\"test\"][\"context\"][idx]) for idx in range(data[\"test\"].shape[0])]\n",
    "answer_true = [data[\"test\"][\"answers\"][idx][\"text\"][0] for idx in range(data[\"test\"].shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.82337636500597, Precision: 0.8175586153353963, Recall: 0.8307474734527724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "results = bertscore.compute(predictions=answer_pred, references=answer_true, lang=\"en\")\n",
    "# Embeddings bases\n",
    "print(f\"F1: {np.array(results['f1']).mean()}, Precision: {np.array(results['precision']).mean()}, Recall: {np.array(results['recall']).mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact': 33.92857142857143,\n",
       " 'f1': 44.40846181641183,\n",
       " 'total': 56,\n",
       " 'HasAns_exact': 33.92857142857143,\n",
       " 'HasAns_f1': 44.40846181641183,\n",
       " 'HasAns_total': 56,\n",
       " 'best_exact': 33.92857142857143,\n",
       " 'best_exact_thresh': 0.0,\n",
       " 'best_f1': 44.40846181641183,\n",
       " 'best_f1_thresh': 0.0}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_v2_metric = evaluate.load(\"squad_v2\")\n",
    "references = [{\"answers\": {\"answer_start\": [answer[\"answer_start\"][0]], \"text\": [answer[\"text\"][0]]}, \"id\": str(id)} for id, answer in zip(data[\"test\"][\"id\"], data[\"test\"][\"answers\"])]\n",
    "predictions = [{\"id\": str(id), \"prediction_text\": answer, \"no_answer_probability\": 0.} for id, answer in zip(data[\"test\"][\"id\"], answer_pred)]\n",
    "results = squad_v2_metric.compute(predictions=predictions, references=references)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.15766505662825686,\n",
       " 'precisions': [0.2857142857142857,\n",
       "  0.19066147859922178,\n",
       "  0.12903225806451613,\n",
       "  0.08791208791208792],\n",
       " 'brevity_penalty': 1.0,\n",
       " 'length_ratio': 1.4951456310679612,\n",
       " 'translation_length': 308,\n",
       " 'reference_length': 206}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu = evaluate.load(\"bleu\")\n",
    "references = [[answer] for answer in answer_true]\n",
    "predictions = answer_pred\n",
    "# N-Gram based\n",
    "results = bleu.compute(predictions=predictions, references=references)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_no_ft = (AutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\").to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_pred_no_ft = [inference_answer(model_no_ft, data[\"test\"][\"question\"][idx], data[\"test\"][\"context\"][idx]) for idx in range(data[\"test\"].shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.3329433873295784, Precision: 0.32331089462552753, Recall: 0.34416832136256353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    }
   ],
   "source": [
    "results = bertscore.compute(predictions=answer_pred_no_ft, references=answer_true, lang=\"en\")\n",
    "# Embeddings bases\n",
    "print(f\"F1: {np.array(results['f1']).mean()}, Precision: {np.array(results['precision']).mean()}, Recall: {np.array(results['recall']).mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact': 0.0,\n",
       " 'f1': 4.7607317985469235,\n",
       " 'total': 56,\n",
       " 'HasAns_exact': 0.0,\n",
       " 'HasAns_f1': 4.7607317985469235,\n",
       " 'HasAns_total': 56,\n",
       " 'best_exact': 0.0,\n",
       " 'best_exact_thresh': 0.0,\n",
       " 'best_f1': 4.7607317985469235,\n",
       " 'best_f1_thresh': 0.0}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "references = [{\"answers\": {\"answer_start\": [answer[\"answer_start\"][0]], \"text\": [answer[\"text\"][0]]}, \"id\": str(id)} for id, answer in zip(data[\"test\"][\"id\"], data[\"test\"][\"answers\"])]\n",
    "predictions = [{\"id\": str(id), \"prediction_text\": answer, \"no_answer_probability\": 0.} for id, answer in zip(data[\"test\"][\"id\"], answer_pred_no_ft)]\n",
    "results = squad_v2_metric.compute(predictions=predictions, references=references)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.016860049000712505,\n",
       " 'precisions': [0.056768558951965066,\n",
       "  0.022988505747126436,\n",
       "  0.012135922330097087,\n",
       "  0.00510204081632653],\n",
       " 'brevity_penalty': 1.0,\n",
       " 'length_ratio': 2.2233009708737863,\n",
       " 'translation_length': 458,\n",
       " 'reference_length': 206}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "references = [[answer] for answer in answer_true]\n",
    "predictions = answer_pred_no_ft\n",
    "# N-Gram based\n",
    "results = bleu.compute(predictions=predictions, references=references)\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

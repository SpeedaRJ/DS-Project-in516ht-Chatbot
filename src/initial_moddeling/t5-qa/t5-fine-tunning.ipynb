{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "from datasets import *\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import ast\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "year = 2022"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference notebooks:\n",
    "\n",
    "https://colab.research.google.com/github/patil-suraj/exploring-T5/blob/master/T5_on_TPU.ipynb#scrollTo=KdmKlMkfcLa0\n",
    "\n",
    "https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization.ipynb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"mrm8488/t5-small-finetuned-squadv2\"  # small model\n",
    "# model_name = \"mrm8488/t5-base-finetuned-squadv2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/rjutr/.cache/huggingface/datasets/csv/default-e3048f1bd60b5c4e/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "Loading cached split indices for dataset at C:\\Users\\rjutr\\.cache\\huggingface\\datasets\\csv\\default-e3048f1bd60b5c4e\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-e96c5bd318352c3f.arrow and C:\\Users\\rjutr\\.cache\\huggingface\\datasets\\csv\\default-e3048f1bd60b5c4e\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-3f782815aab69336.arrow\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset from file and split it into train and test datasets\n",
    "data = load_dataset('csv', data_files=\"../../data/clean/sustainability-report-2022-squad-format.csv\",\n",
    "                    delimiter=\";\", split='train').train_test_split(test_size=0.3, shuffle=True, seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\rjutr\\.cache\\huggingface\\datasets\\csv\\default-e3048f1bd60b5c4e\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-0e58ef57ff25bb58.arrow\n",
      "Loading cached processed dataset at C:\\Users\\rjutr\\.cache\\huggingface\\datasets\\csv\\default-e3048f1bd60b5c4e\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-944b27bfd97247d4.arrow\n",
      "Loading cached processed dataset at C:\\Users\\rjutr\\.cache\\huggingface\\datasets\\csv\\default-e3048f1bd60b5c4e\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-d92cbfe0185ed3c0.arrow\n",
      "Loading cached processed dataset at C:\\Users\\rjutr\\.cache\\huggingface\\datasets\\csv\\default-e3048f1bd60b5c4e\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-e00fcfca42a8136e.arrow\n"
     ]
    }
   ],
   "source": [
    "# Reformat the train and test set such as they adhere to the SQuAD format (reading from cvs loads strings not objects as expected)\n",
    "data[\"test\"] = data[\"test\"].map(\n",
    "    lambda example: ast.literal_eval(example[\"answers\"]))\n",
    "data[\"test\"] = data[\"test\"].map(lambda example: {\"question\": example[\"question\"], \"context\": example[\"context\"], \"answers\": {\n",
    "                                \"text\": example[\"text\"], \"answer_start\": example[\"answer_start\"]}})\n",
    "data[\"test\"] = data[\"test\"].remove_columns([\"text\", \"answer_start\"])\n",
    "\n",
    "data[\"train\"] = data[\"train\"].map(\n",
    "    lambda example: ast.literal_eval(example[\"answers\"]))\n",
    "data[\"train\"] = data[\"train\"].map(lambda example: {\"question\": example[\"question\"], \"context\": example[\"context\"], \"answers\": {\n",
    "                                  \"text\": example[\"text\"], \"answer_start\": example[\"answer_start\"]}})\n",
    "data[\"train\"] = data[\"train\"].remove_columns([\"text\", \"answer_start\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'index': 358,\n",
       " 'question': 'When was the first banking institution established?',\n",
       " 'context': 'on is based on people,banking knowledge and culture reaching back to 1820 whenthe first banking institution  the Carniola Savings Bank  was',\n",
       " 'answers': {'answer_start': [69], 'text': ['1820']},\n",
       " 'id': 143}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"train\"][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the examples in input and target text format and the eos token at the end\n",
    "def add_eos_to_examples(example):\n",
    "    example['input_text'] = 'question: %s  context: %s </s>' % (\n",
    "        example['question'], example['context'])\n",
    "    example['target_text'] = '%s </s>' % example['answers']['text'][0]\n",
    "    return example\n",
    "\n",
    "# tokenize the examples\n",
    "\n",
    "\n",
    "def convert_to_features(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples['input_text'], pad_to_max_length=True, max_length=512, truncation=True)\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            examples['target_text'], pad_to_max_length=True, max_length=128, truncation=True)\n",
    "        temp = np.array(labels[\"input_ids\"])\n",
    "        temp[temp == tokenizer.pad_token_id] = -100\n",
    "        labels[\"input_ids\"] = temp.tolist()\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8302e9e7f259429f8daa5748bc0d4637",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/248 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9f71cf21f0041d68734e70992616efd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/248 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rjutr\\miniconda3\\envs\\project_ds_2\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "c:\\Users\\rjutr\\miniconda3\\envs\\project_ds_2\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3596: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f662b18097a4e73ab864be8763a8954",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/107 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "456a38e3dcee431eb65a5b593af078d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/107 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data, test_data = data[\"train\"], data[\"test\"]\n",
    "\n",
    "train_data = train_data.map(add_eos_to_examples, load_from_cache_file=False)\n",
    "train_data = train_data.map(\n",
    "    convert_to_features, batched=True, load_from_cache_file=False)\n",
    "\n",
    "test_data = test_data.map(add_eos_to_examples, load_from_cache_file=False)\n",
    "test_data = test_data.map(\n",
    "    convert_to_features, batched=True, load_from_cache_file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'index': 358,\n",
       " 'question': 'When was the first banking institution established?',\n",
       " 'context': 'on is based on people,banking knowledge and culture reaching back to 1820 whenthe first banking institution  the Carniola Savings Bank  was',\n",
       " 'answers': {'answer_start': [69], 'text': ['1820']},\n",
       " 'id': 143,\n",
       " 'input_text': 'question: When was the first banking institution established?  context: on is based on people,banking knowledge and culture reaching back to 1820 whenthe first banking institution  the Carniola Savings Bank  was </s>',\n",
       " 'target_text': '1820 </s>',\n",
       " 'input_ids': [822,\n",
       "  10,\n",
       "  366,\n",
       "  47,\n",
       "  8,\n",
       "  166,\n",
       "  8175,\n",
       "  6568,\n",
       "  2127,\n",
       "  58,\n",
       "  2625,\n",
       "  10,\n",
       "  30,\n",
       "  19,\n",
       "  3,\n",
       "  390,\n",
       "  30,\n",
       "  151,\n",
       "  6,\n",
       "  4739,\n",
       "  53,\n",
       "  1103,\n",
       "  11,\n",
       "  1543,\n",
       "  7232,\n",
       "  223,\n",
       "  12,\n",
       "  507,\n",
       "  1755,\n",
       "  116,\n",
       "  532,\n",
       "  166,\n",
       "  8175,\n",
       "  6568,\n",
       "  8,\n",
       "  1184,\n",
       "  29,\n",
       "  23578,\n",
       "  19063,\n",
       "  7,\n",
       "  1925,\n",
       "  47,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'labels': [507,\n",
       "  1755,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "\n",
    "name = model_name.split(\"/\")[-1]\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=f\"{name}-finetuned-NLB-QA-{year}\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=25,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "# with this batch size the base model fits on a GPU with 8GB of memory\n",
    "# training_args = Seq2SeqTrainingArguments(\n",
    "#     output_dir = f\"./models\",\n",
    "#     evaluation_strategy = \"epoch\",\n",
    "#     learning_rate=2e-5,\n",
    "#     per_device_train_batch_size=4,\n",
    "#     per_device_eval_batch_size=4,\n",
    "#     weight_decay=0.01,\n",
    "#     save_total_limit=3,\n",
    "#     num_train_epochs=25,\n",
    "#     predict_with_generate=True,\n",
    "#     fp16=True,\n",
    "#     push_to_hub=False\n",
    "# )\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=test_data,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rjutr\\miniconda3\\envs\\project_ds_2\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "794b5b5bb8ec443190779e103c2d31d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/775 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a68cb3d14adc425cbd10945a86ef77b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8605215549468994, 'eval_runtime': 0.547, 'eval_samples_per_second': 195.613, 'eval_steps_per_second': 25.594, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71b8355e104c4f90bed73b4515badf55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.465985894203186, 'eval_runtime': 0.5327, 'eval_samples_per_second': 200.854, 'eval_steps_per_second': 26.28, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc294e72283a45ae8563a07fb5812128",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1761937141418457, 'eval_runtime': 0.5292, 'eval_samples_per_second': 202.197, 'eval_steps_per_second': 26.456, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ea86738d5304c6284b2f0d48e0a7cbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8798961639404297, 'eval_runtime': 0.5279, 'eval_samples_per_second': 202.706, 'eval_steps_per_second': 26.522, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2225d63d21e6407e998519bcc0867895",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5622873902320862, 'eval_runtime': 0.5421, 'eval_samples_per_second': 197.369, 'eval_steps_per_second': 25.824, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf3108e9e8864e409a4a8b355c660cce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3936600983142853, 'eval_runtime': 0.5567, 'eval_samples_per_second': 192.221, 'eval_steps_per_second': 25.15, 'epoch': 6.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "062ae3d63d844cf0868f89a4f19d7882",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3170520067214966, 'eval_runtime': 0.545, 'eval_samples_per_second': 196.327, 'eval_steps_per_second': 25.688, 'epoch': 7.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4209a3873d07430fa934f5e6dc13b18c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.26753655076026917, 'eval_runtime': 0.578, 'eval_samples_per_second': 185.121, 'eval_steps_per_second': 24.221, 'epoch': 8.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b68261c42f044c3978e882377c05934",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2362598329782486, 'eval_runtime': 0.5486, 'eval_samples_per_second': 195.029, 'eval_steps_per_second': 25.518, 'epoch': 9.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b98eba885abd44d0866f598b6dfe0d62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.21938280761241913, 'eval_runtime': 0.5457, 'eval_samples_per_second': 196.095, 'eval_steps_per_second': 25.657, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ed76b22446a48bab8f5e3cf163e8548",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2066628783941269, 'eval_runtime': 0.5408, 'eval_samples_per_second': 197.846, 'eval_steps_per_second': 25.886, 'epoch': 11.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ef8856281a8466884b7a0657d89c446",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.20078468322753906, 'eval_runtime': 0.5582, 'eval_samples_per_second': 191.678, 'eval_steps_per_second': 25.079, 'epoch': 12.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df227d948e8845fdac6adc1b0ddde7bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.19411641359329224, 'eval_runtime': 0.5386, 'eval_samples_per_second': 198.679, 'eval_steps_per_second': 25.995, 'epoch': 13.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4310c689cacd4c3d80feea3dbfb270c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1893462985754013, 'eval_runtime': 0.577, 'eval_samples_per_second': 185.437, 'eval_steps_per_second': 24.263, 'epoch': 14.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8634c31fda94e8a8514ff6c0df30731",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.18639765679836273, 'eval_runtime': 0.55, 'eval_samples_per_second': 194.546, 'eval_steps_per_second': 25.455, 'epoch': 15.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27c7dbb3d1b34526a4047337e9e26142",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.18122951686382294, 'eval_runtime': 0.5354, 'eval_samples_per_second': 199.847, 'eval_steps_per_second': 26.148, 'epoch': 16.0}\n",
      "{'loss': 0.7045, 'learning_rate': 7.2000000000000005e-06, 'epoch': 16.13}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c24bfcd971d496d97ba54bd3d200e4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.17755372822284698, 'eval_runtime': 0.5571, 'eval_samples_per_second': 192.057, 'eval_steps_per_second': 25.129, 'epoch': 17.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f8a3035fe8841b7bf13d1dacce02c3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.17584510147571564, 'eval_runtime': 0.5589, 'eval_samples_per_second': 191.443, 'eval_steps_per_second': 25.049, 'epoch': 18.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "437b4945c8d6409e8b639028d9c65bfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.17383332550525665, 'eval_runtime': 0.576, 'eval_samples_per_second': 185.764, 'eval_steps_per_second': 24.306, 'epoch': 19.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a8cdede13594c54ae23cafd6d3281a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.17242954671382904, 'eval_runtime': 0.566, 'eval_samples_per_second': 189.046, 'eval_steps_per_second': 24.735, 'epoch': 20.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68b5da79cb734ebb95669f8580e315d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.17186328768730164, 'eval_runtime': 0.5619, 'eval_samples_per_second': 190.44, 'eval_steps_per_second': 24.917, 'epoch': 21.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64a1cc52c7034176bd8cd16543a6532a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.17117920517921448, 'eval_runtime': 0.5647, 'eval_samples_per_second': 189.482, 'eval_steps_per_second': 24.792, 'epoch': 22.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b91c128cda0c4c8fa7537447b7a14c6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.17101497948169708, 'eval_runtime': 0.5546, 'eval_samples_per_second': 192.929, 'eval_steps_per_second': 25.243, 'epoch': 23.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fcfa58039454d34926e890affeed116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.17068082094192505, 'eval_runtime': 0.554, 'eval_samples_per_second': 193.14, 'eval_steps_per_second': 25.271, 'epoch': 24.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "762fc2a38af94374a2551a27e5d8f182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.17061103880405426, 'eval_runtime': 0.5495, 'eval_samples_per_second': 194.717, 'eval_steps_per_second': 25.477, 'epoch': 25.0}\n",
      "{'train_runtime': 124.3145, 'train_samples_per_second': 49.874, 'train_steps_per_second': 6.234, 'train_loss': 0.5058726156911543, 'epoch': 25.0}\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.save_model(f\"{name}-finetuned-NLB-QA-{year}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(question, context):\n",
    "    input_text = \"question: %s  context: %s\" % (question, context)\n",
    "    features = tokenizer([input_text], return_tensors='pt')\n",
    "\n",
    "    output = model.generate(\n",
    "        input_ids=features['input_ids'], attention_mask=features['attention_mask'])\n",
    "\n",
    "    return tokenizer.decode(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are absolute emissions and emissions intensity in line with? \n",
      "Context: ly publishing absolute emissions and emissionsintensity in line with best practice and, within a year ofsetting targets, disclosing progress against a  \n",
      "Answer: best practice\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Squad model answer: <pad> best practice</s>\n",
      "Our model answer: <pad> best practice </s>\n"
     ]
    }
   ],
   "source": [
    "question = test_data[0][\"question\"]\n",
    "context = test_data[0][\"context\"]\n",
    "answer = test_data[0][\"answers\"][\"text\"][0]\n",
    "print(f\"Question: {question} \\nContext: {context} \\nAnswer: {answer}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "print(f\"Squad model answer: {get_answer(question, context)}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    f\"./{name}-finetuned-NLB-QA-{year}\", local_files_only=True)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    f\"./{name}-finetuned-NLB-QA-{year}\", local_files_only=True)\n",
    "print(f\"Our model answer: {get_answer(question, context)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n"
     ]
    }
   ],
   "source": [
    "answers = [temp[\"answers\"][\"text\"][0] for temp in test_data]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "squad_answers = [get_answer(question, context) for question, context in zip(\n",
    "    test_data[\"question\"], test_data[\"context\"])]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    f\"./{name}-finetuned-NLB-QA-{year}\", local_files_only=True)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    f\"./{name}-finetuned-NLB-QA-{year}\", local_files_only=True)\n",
    "our_answers = [get_answer(question, context) for question, context in zip(\n",
    "    test_data[\"question\"], test_data[\"context\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad> best practice</s>',\n",
       " '<pad> Magda</s>',\n",
       " '<pad> more than 400</s>',\n",
       " '<pad> Sustainability Development unit</s>',\n",
       " '<pad> None</s>',\n",
       " '<pad> Top Employer award</s>',\n",
       " '<pad> annual performance evaluation</s>',\n",
       " '<pad> three levels</s>',\n",
       " '<pad> six</s>',\n",
       " '<pad>ESG risks and opportunities</s>',\n",
       " '<pad> six</s>',\n",
       " '<pad> 2022</s>',\n",
       " '<pad> 5</s>',\n",
       " '<pad> leading initiatives within theassociations</s>',\n",
       " '<pad> None</s>',\n",
       " '<pad> the EBRD</s>',\n",
       " '<pad> 100%</s>',\n",
       " '<pad> 10%</s>',\n",
       " '<pad> Relationship Manager</s>',\n",
       " '<pad> None</s>',\n",
       " '<pad> three</s>',\n",
       " '<pad> managerial, professional or young talent</s>',\n",
       " '<pad> None</s>',\n",
       " '<pad> 254</s>',\n",
       " '<pad> composition, performance, potential conflict ofinterest</s>',\n",
       " '<pad> 2022</s>',\n",
       " '<pad> 14.5%</s>',\n",
       " '<pad> 3</s>',\n",
       " '<pad> fromzero carbon energysourceoperationalcarbonfootprintreductiontrees</s>',\n",
       " '<pad> 6%</s>',\n",
       " '<pad> 2023</s>',\n",
       " '<pad> 50%</s>',\n",
       " '<pad> ILO standards</s>',\n",
       " '<pad> 2050</s>',\n",
       " '<pad> ransparentprocurement</s>',\n",
       " '<pad> The risk management function</s>',\n",
       " '<pad> 2020</s>',\n",
       " '<pad> ESG criteria</s>',\n",
       " '<pad> None</s>',\n",
       " '<pad> 10</s>',\n",
       " '<pad> Joint ESMA and EBA Guidelines</s>',\n",
       " '<pad> 10</s>',\n",
       " '<pad> Environmental andSocial Management System</s>',\n",
       " '<pad> 2022</s>',\n",
       " '<pad> 136</s>',\n",
       " '<pad> 2021</s>',\n",
       " '<pad> unified</s>',\n",
       " '<pad> 2022</s>',\n",
       " '<pad> European Securities and MarketsAuthority</s>',\n",
       " '<pad> None</s>',\n",
       " '<pad> 2022</s>',\n",
       " '<pad> The variable part</s>',\n",
       " '<pad> 17</s>',\n",
       " '<pad> None</s>',\n",
       " '<pad> 33%</s>',\n",
       " '<pad> Internal Capital AdequacyAssessment Process</s>',\n",
       " '<pad> Code of Conduct of NLB Group</s>',\n",
       " '<pad> None</s>',\n",
       " '<pad> 2022</s>',\n",
       " '<pad> 300</s>',\n",
       " '<pad> 1,690</s>',\n",
       " '<pad> None</s>',\n",
       " '<pad> 3</s>',\n",
       " '<pad> three modules</s>',\n",
       " '<pad> less than 2% of data in baseline year</s>',\n",
       " '<pad> strong promotion of sports</s>',\n",
       " '<pad> None</s>',\n",
       " '<pad> 36%</s>',\n",
       " '<pad> 2022</s>',\n",
       " '<pad> 22</s>',\n",
       " '<pad> 200</s>',\n",
       " '<pad> aims to harmonise credit andinvestment portfolios by r</s>',\n",
       " '<pad> three</s>',\n",
       " '<pad> None</s>',\n",
       " '<pad> 2023</s>',\n",
       " '<pad> 13</s>',\n",
       " '<pad> None</s>',\n",
       " '<pad> None</s>',\n",
       " '<pad> controlled introduction</s>',\n",
       " '<pad> Excel, Word, PPT, Power BI</s>',\n",
       " '<pad> None</s>',\n",
       " '<pad> two</s>',\n",
       " '<pad> esidential mortgages</s>',\n",
       " '<pad> Komercijalna banka Beograd</s>',\n",
       " '<pad> 43</s>',\n",
       " '<pad> green transition</s>',\n",
       " '<pad> 2022</s>',\n",
       " '<pad> 2022</s>',\n",
       " '<pad> 866</s>',\n",
       " '<pad> corruption and bribery</s>',\n",
       " '<pad> to identify all relevant factors and measures that contribute to greater and more successful employee engagement</s>',\n",
       " '<pad> 2022</s>',\n",
       " '<pad> 2021</s>',\n",
       " '<pad> card services</s>',\n",
       " '<pad> 2020</s>',\n",
       " '<pad> six</s>',\n",
       " '<pad> None</s>',\n",
       " '<pad> None</s>',\n",
       " '<pad> risk drivers of the existing type of risks</s>',\n",
       " '<pad> The NLB Group</s>',\n",
       " '<pad> 3</s>',\n",
       " '<pad> 1452,8</s>',\n",
       " '<pad> 1,452</s>',\n",
       " '<pad> 2020</s>',\n",
       " '<pad> None</s>',\n",
       " '<pad> sixcountries</s>',\n",
       " '<pad> low-carbon circulareconomy</s>']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad> best practice </s>',\n",
       " '<pad> Magda </s>',\n",
       " '<pad> 400 </s>',\n",
       " '<pad> Sustainability Development unit </s>',\n",
       " '<pad> Cyberse</s>',\n",
       " '<pad> Top Employer award </s>',\n",
       " '<pad> annual performance evaluation</s>',\n",
       " '<pad> three levels</s>',\n",
       " '<pad> six</s>',\n",
       " '<pad>ESG risks and opportunities </s>',\n",
       " '<pad> six </s>',\n",
       " '<pad> 2022 </s>',\n",
       " '<pad> 5 </s>',\n",
       " '<pad> leading initiatives within theassociations </s>',\n",
       " '<pad>2022 </s>',\n",
       " '<pad> EBRD </s>',\n",
       " '<pad> 100%</s>',\n",
       " '<pad> 10% </s>',\n",
       " '<pad> Relationship Manager </s>',\n",
       " '<pad> ESG regulatory developments </s>',\n",
       " '<pad> three</s>',\n",
       " '<pad> managerial, professional or young talent </s>',\n",
       " '<pad> ESMA </s>',\n",
       " '<pad> 254 </s>',\n",
       " '<pad> its composition, performance, potential conflict ofinterest of individual members of the Supervis</s>',\n",
       " '<pad> 2022 </s>',\n",
       " '<pad> 14.5%.</s>',\n",
       " '<pad> 3 </s>',\n",
       " '<pad> new sustainable carbon energysourceoperationalcarbonfootprintreductiontrees</s>',\n",
       " '<pad> 6%,</s>',\n",
       " '<pad> 2023 </s>',\n",
       " '<pad> 50% </s>',\n",
       " '<pad> ILO standards</s>',\n",
       " '<pad> 2050 or earlier</s>',\n",
       " '<pad> ransparentprocurement procedure </s>',\n",
       " '<pad> The risk management function</s>',\n",
       " '<pad> 2020 </s>',\n",
       " '<pad> ESG criteria</s>',\n",
       " '<pad> To reduce water consump-tion</s>',\n",
       " '<pad> 10 </s>',\n",
       " '<pad> Joint ESMA and EBA Guidelines </s>',\n",
       " '<pad> 10 </s>',\n",
       " '<pad> Environmental andSocial Management System </s>',\n",
       " '<pad> 2022 </s>',\n",
       " '<pad> 136 </s>',\n",
       " '<pad> 2021 </s>',\n",
       " '<pad> a unified way</s>',\n",
       " '<pad> 2022 </s>',\n",
       " '<pad> European Securities and MarketsAuthority </s>',\n",
       " '<pad> 50% </s>',\n",
       " '<pad> 2022 </s>',\n",
       " '<pad> variable part </s>',\n",
       " '<pad> 17 </s>',\n",
       " '<pad> School ofEconomics </s>',\n",
       " '<pad> 40% </s>',\n",
       " '<pad> Internal Capital AdequacyAssessment Process </s>',\n",
       " '<pad> Code of Conduct of NLB Group</s>',\n",
       " '<pad> Policy of Business ContinuityManagement</s>',\n",
       " '<pad> 2022 </s>',\n",
       " '<pad> 300 </s>',\n",
       " '<pad> 82,280 </s>',\n",
       " '<pad> NACE Nomenclature of Economic ActivitiesNFRD </s>',\n",
       " '<pad> 3 </s>',\n",
       " '<pad> three </s>',\n",
       " '<pad> less than 2% of data in baseline year </s>',\n",
       " '<pad> strong promotion of sports </s>',\n",
       " '<pad> reducing the carbon footprint </s>',\n",
       " '<pad> 36% </s>',\n",
       " '<pad> 2022 </s>',\n",
       " '<pad> 22 </s>',\n",
       " '<pad> around 200 </s>',\n",
       " '<pad> Net Zero BankingAlliance </s>',\n",
       " '<pad> three </s>',\n",
       " '<pad> 2022 </s>',\n",
       " '<pad> 2023 </s>',\n",
       " '<pad> 13 </s>',\n",
       " '<pad> electronically via the Whistler web application</s>',\n",
       " '<pad> dESMS Officers of NLB Group </s>',\n",
       " '<pad> controlled introduction </s>',\n",
       " '<pad> Excel, Word, PPT, Power BI, etc.</s>',\n",
       " '<pad> fossil fuels </s>',\n",
       " '<pad> two </s>',\n",
       " '<pad> esidential mortgages</s>',\n",
       " '<pad> Komercijalna banka Beograd </s>',\n",
       " '<pad> 8,228 </s>',\n",
       " '<pad> green transition </s>',\n",
       " '<pad> 2022 </s>',\n",
       " '<pad> 2022 </s>',\n",
       " '<pad> 866 </s>',\n",
       " '<pad> corruption and bribery</s>',\n",
       " '<pad> to identify all relevant factors and measures that contribute to greater and more successful employee engagement </s>',\n",
       " '<pad> 2022 </s>',\n",
       " '<pad> 2021 </s>',\n",
       " '<pad> card services</s>',\n",
       " '<pad> 2020</s>',\n",
       " '<pad> six</s>',\n",
       " '<pad> NLB Risk Management Statement</s>',\n",
       " '<pad> 32 </s>',\n",
       " '<pad> risk drivers of the existing type of risks, such as credit,liquidity, market and operational',\n",
       " '<pad> NLB Group </s>',\n",
       " '<pad> 3 </s>',\n",
       " '<pad> 1452,8 </s>',\n",
       " '<pad> 1,452</s>',\n",
       " '<pad> 2020 </s>',\n",
       " '<pad> 70% </s>',\n",
       " '<pad> six </s>',\n",
       " '<pad> a low-carbon circulareconomy</s>']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Squad\n",
      "F1: 0.868855295894302, Precision: 0.8768738697622424, Recall: 0.8617404877582443\n",
      "Our model\n",
      "F1: 0.8580790713568714, Precision: 0.8443797642939559, Recall: 0.8726573970830329\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "results = bertscore.compute(\n",
    "    predictions=squad_answers, references=answers, lang=\"en\")\n",
    "# Embeddings bases evaluation\n",
    "print(\n",
    "    f\"Squad\\nF1: {np.array(results['f1']).mean()}, Precision: {np.array(results['precision']).mean()}, Recall: {np.array(results['recall']).mean()}\")\n",
    "\n",
    "results = bertscore.compute(predictions=our_answers,\n",
    "                            references=answers, lang=\"en\")\n",
    "# Embeddings bases evaluation\n",
    "print(\n",
    "    f\"Our model\\nF1: {np.array(results['f1']).mean()}, Precision: {np.array(results['precision']).mean()}, Recall: {np.array(results['recall']).mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Squad\n",
      "{'bleu': 0.1020040212593282, 'precisions': [0.18302658486707565, 0.1113662456946039, 0.08115183246073299, 0.0654490106544901], 'brevity_penalty': 1.0, 'length_ratio': 3.5434782608695654, 'translation_length': 978, 'reference_length': 276}\n",
      "Our model\n",
      "{'bleu': 0.11752063190477298, 'precisions': [0.21484375, 0.13304252998909488, 0.09382716049382717, 0.07112375533428165], 'brevity_penalty': 1.0, 'length_ratio': 3.710144927536232, 'translation_length': 1024, 'reference_length': 276}\n"
     ]
    }
   ],
   "source": [
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "results = bleu.compute(predictions=squad_answers, references=answers)\n",
    "print(f\"Squad\\n{results}\")\n",
    "results = bleu.compute(predictions=our_answers, references=answers)\n",
    "print(f\"Our model\\n{results}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

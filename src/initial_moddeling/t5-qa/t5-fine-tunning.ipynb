{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "from datasets import *\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import ast\n",
    "\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference notebooks:\n",
    "\n",
    "https://colab.research.google.com/github/patil-suraj/exploring-T5/blob/master/T5_on_TPU.ipynb#scrollTo=KdmKlMkfcLa0\n",
    "\n",
    "https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization.ipynb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mrm8488/t5-small-finetuned-squadv2\" # small model\n",
    "# model_name = \"mrm8488/t5-base-finetuned-squadv2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from file and split it into train and test datasets\n",
    "data = load_dataset('csv', data_files=\"../../data/clean/sustainability-report-2020-squad-format.csv\", delimiter=\";\", split='train').train_test_split(test_size=0.3, shuffle=True, seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat the train and test set such as they adhere to the SQuAD format (reading from cvs loads strings not objects as expected)\n",
    "data[\"test\"] = data[\"test\"].map(lambda example: ast.literal_eval(example[\"answers\"]))\n",
    "data[\"test\"] = data[\"test\"].map(lambda example: {\"question\": example[\"question\"], \"context\": example[\"context\"], \"answers\": {\"text\": example[\"text\"], \"answer_start\": example[\"answer_start\"]}})\n",
    "data[\"test\"] = data[\"test\"].remove_columns([\"text\", \"answer_start\"])\n",
    "\n",
    "data[\"train\"] = data[\"train\"].map(lambda example: ast.literal_eval(example[\"answers\"]))\n",
    "data[\"train\"] = data[\"train\"].map(lambda example: {\"question\": example[\"question\"], \"context\": example[\"context\"], \"answers\": {\"text\": example[\"text\"], \"answer_start\": example[\"answer_start\"]}})\n",
    "data[\"train\"] = data[\"train\"].remove_columns([\"text\", \"answer_start\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"train\"][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the examples in input and target text format and the eos token at the end \n",
    "def add_eos_to_examples(example):\n",
    "    example['input_text'] = 'question: %s  context: %s </s>' % (example['question'], example['context'])\n",
    "    example['target_text'] = '%s </s>' % example['answers']['text'][0]\n",
    "    return example\n",
    "\n",
    "# tokenize the examples\n",
    "def convert_to_features(examples):\n",
    "    model_inputs = tokenizer(examples['input_text'], pad_to_max_length=True, max_length=512, truncation=True)\n",
    "    \n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples['target_text'], pad_to_max_length=True, max_length=128, truncation=True)\n",
    "        temp = np.array(labels[\"input_ids\"])\n",
    "        temp[temp == tokenizer.pad_token_id] = -100\n",
    "        labels[\"input_ids\"] = temp.tolist()\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = data[\"train\"], data[\"test\"]\n",
    "\n",
    "train_data = train_data.map(add_eos_to_examples, load_from_cache_file=False)\n",
    "train_data = train_data.map(convert_to_features, batched=True, load_from_cache_file=False)\n",
    "\n",
    "test_data = test_data.map(add_eos_to_examples, load_from_cache_file=False)\n",
    "test_data = test_data.map(convert_to_features, batched=True, load_from_cache_file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "\n",
    "name = model_name.split(\"/\")[-1]\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir = f\"./models\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=25,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "# with this batch size the base model fits on a GPU with 8GB of memory\n",
    "# training_args = Seq2SeqTrainingArguments(\n",
    "#     output_dir = f\"./models\",\n",
    "#     evaluation_strategy = \"epoch\",\n",
    "#     learning_rate=2e-5,\n",
    "#     per_device_train_batch_size=4,\n",
    "#     per_device_eval_batch_size=4,\n",
    "#     weight_decay=0.01,\n",
    "#     save_total_limit=3,\n",
    "#     num_train_epochs=25,\n",
    "#     predict_with_generate=True,\n",
    "#     fp16=True,\n",
    "#     push_to_hub=False\n",
    "# )\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=test_data,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "trainer.save_model(f\"./models/{name}-finetuned-NLB-QA\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(question, context):\n",
    "    input_text = \"question: %s  context: %s\" % (question, context)\n",
    "    features = tokenizer([input_text], return_tensors='pt')\n",
    "\n",
    "    output = model.generate(input_ids=features['input_ids'], attention_mask=features['attention_mask'])\n",
    "\n",
    "    return tokenizer.decode(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = test_data[0][\"question\"]\n",
    "context = test_data[0][\"context\"]\n",
    "answer = test_data[0][\"answers\"][\"text\"][0]\n",
    "print(f\"Question: {question} \\nContext: {context} \\nAnswer: {answer}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "print(f\"Squad model answer: {get_answer(question, context)}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(f\"./models/{name}-finetuned-NLB-QA\", local_files_only=True)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(f\"./models/{name}-finetuned-NLB-QA\", local_files_only=True)\n",
    "print(f\"Our model answer: {get_answer(question, context)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = [temp[\"answers\"][\"text\"][0] for temp in test_data]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "squad_answers = [get_answer(question, context) for question, context in zip(test_data[\"question\"], test_data[\"context\"])]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(f\"./models/{name}-finetuned-NLB-QA\", local_files_only=True)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(f\"./models/{name}-finetuned-NLB-QA\", local_files_only=True)\n",
    "our_answers = [get_answer(question, context) for question, context in zip(test_data[\"question\"], test_data[\"context\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "results = bertscore.compute(predictions=squad_answers, references=answers, lang=\"en\")\n",
    "# Embeddings bases evaluation\n",
    "print(f\"Squad\\nF1: {np.array(results['f1']).mean()}, Precision: {np.array(results['precision']).mean()}, Recall: {np.array(results['recall']).mean()}\")\n",
    "\n",
    "results = bertscore.compute(predictions=our_answers, references=answers, lang=\"en\")\n",
    "# Embeddings bases evaluation\n",
    "print(f\"Our model\\nF1: {np.array(results['f1']).mean()}, Precision: {np.array(results['precision']).mean()}, Recall: {np.array(results['recall']).mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "results = bleu.compute(predictions=squad_answers, references=answers)\n",
    "print(f\"Squad\\n{results}\")\n",
    "results = bleu.compute(predictions=our_answers, references=answers)\n",
    "print(f\"Our model\\n{results}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

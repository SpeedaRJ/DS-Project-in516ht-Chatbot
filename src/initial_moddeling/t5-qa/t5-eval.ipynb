{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, set_seed\n",
    "from datasets import *\n",
    "import numpy as np\n",
    "import torch\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "\n",
    "year = 2020\n",
    "# year = 2022\n",
    "\n",
    "local_models_path = '../../data/models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/Luka/.cache/huggingface/datasets/csv/default-d8382661cd597e83/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "Loading cached split indices for dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-d8382661cd597e83\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-c261d5613d28d856.arrow and C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-d8382661cd597e83\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-e61829c1e4a24b65.arrow\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset from file and split it into train and test datasets\n",
    "data = load_dataset('csv', data_files=f\"../../data/clean/sustainability-report-{year}-squad-format.csv\",\n",
    "                    delimiter=\";\", split='train').train_test_split(test_size=0.3, shuffle=True, seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-d8382661cd597e83\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-0b15501cefb41ff7.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-d8382661cd597e83\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-e4de42d02343959f.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['382',\n",
       " 'green/sustainability financing',\n",
       " '1',\n",
       " '2021',\n",
       " 'law, collectiveagreements and internal regulations',\n",
       " 'Social and EnvironmentalPolicy',\n",
       " 'equal opportunities, justice',\n",
       " 'a visit by Santa Claus',\n",
       " '2020',\n",
       " 'three',\n",
       " '30th December 2020',\n",
       " '2020',\n",
       " '6.7%,',\n",
       " 'Komercijalna Banka a.d. Beograd',\n",
       " '2020',\n",
       " '2021',\n",
       " 'Bogdan DarmanoviÄ‡',\n",
       " '4,769',\n",
       " 'World Institute forSustainability and Ethics in Rising Economies',\n",
       " '2018',\n",
       " '2020',\n",
       " '23 million EUR',\n",
       " '307',\n",
       " 'a higher quality of life of the wider society',\n",
       " 'EUR 340 million',\n",
       " 'Retail Banking in Slovenia, Corporate Bankingin Slovenia, and Strategic Foreign Markets',\n",
       " '30.12.2020',\n",
       " '69% women and 31% men',\n",
       " 'More than 200',\n",
       " '31%',\n",
       " 'corruption and bribery',\n",
       " '307',\n",
       " '2.11million',\n",
       " '17,297',\n",
       " '58%',\n",
       " '45',\n",
       " '2017',\n",
       " 'annually',\n",
       " 'CRS',\n",
       " '2019',\n",
       " '97%',\n",
       " 'EUR 340 million',\n",
       " '94',\n",
       " 'Beograd',\n",
       " '2,914',\n",
       " '4 Sep',\n",
       " '2%.',\n",
       " '69% women and 31% men',\n",
       " 'to invest in a systematicdevelopment of employees',\n",
       " '17,295',\n",
       " 'by e-mail, via the website,and social networks',\n",
       " '55%',\n",
       " '307',\n",
       " '2,914',\n",
       " '2020',\n",
       " '11']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reformat the train and test set such as they adhere to the SQuAD format (reading from cvs loads strings not objects as expected)\n",
    "data[\"test\"] = data[\"test\"].map(\n",
    "    lambda example: ast.literal_eval(example[\"answers\"]))\n",
    "data[\"test\"] = data[\"test\"].map(lambda example: {\"question\": example[\"question\"], \"context\": example[\"context\"], \"answers\": {\n",
    "                                \"text\": example[\"text\"], \"answer_start\": example[\"answer_start\"]}})\n",
    "data[\"test\"] = data[\"test\"].remove_columns([\"text\", \"answer_start\"])\n",
    "\n",
    "test_data = data[\"test\"]\n",
    "gt_answers = [temp[\"answers\"][\"text\"][0] for temp in test_data]\n",
    "gt_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(question, context, tokenizer, model):\n",
    "    input_text = \"question: %s  context: %s\" % (question, context)\n",
    "    features = tokenizer([input_text], return_tensors='pt')\n",
    "\n",
    "    output = model.generate(input_ids=features['input_ids'], attention_mask=features['attention_mask'])\n",
    "\n",
    "    return tokenizer.decode(output[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIMPLE EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "bleu = evaluate.load(\"bleu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "c:\\Users\\Luka\\miniconda3\\envs\\project_ds\\lib\\site-packages\\transformers\\generation\\utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bertscore\n",
      "F1: 0.8861216104456356, Precision: 0.8740572397197995, Recall: 0.8988280519843102\n",
      "Bleu\n",
      "{'bleu': 0.14932377828599544, 'precisions': [0.2303473491773309, 0.15885947046843177, 0.12873563218390804, 0.10554089709762533], 'brevity_penalty': 1.0, 'length_ratio': 3.529032258064516, 'translation_length': 547, 'reference_length': 155}\n"
     ]
    }
   ],
   "source": [
    "model_name = \"mrm8488/t5-small-finetuned-squadv2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "answers = [get_answer(question, context, tokenizer, model) for question, context in zip(test_data[\"question\"], test_data[\"context\"])]\n",
    "\n",
    "# bertscore\n",
    "results = bertscore.compute(predictions=answers, references=gt_answers, lang=\"en\")\n",
    "print(f\"Bertscore\\nF1: {np.array(results['f1']).mean()}, Precision: {np.array(results['precision']).mean()}, Recall: {np.array(results['recall']).mean()}\")\n",
    "\n",
    "# bleu\n",
    "results = bleu.compute(predictions=answers, references=gt_answers)\n",
    "print(f\"Bleu\\n{results}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small - finetuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bertscore\n",
      "F1: 0.8897274541003364, Precision: 0.8763338146465165, Recall: 0.9038189905030387\n",
      "Bleu\n",
      "{'bleu': 0.1568508288241227, 'precisions': [0.24150268336314848, 0.16699801192842942, 0.13646532438478748, 0.10997442455242967], 'brevity_penalty': 1.0, 'length_ratio': 3.606451612903226, 'translation_length': 559, 'reference_length': 155}\n"
     ]
    }
   ],
   "source": [
    "model_name = f\"{local_models_path}/t5-small-finetuned-squadv2-finetuned-NLB-QA-{year}\" # TODO: path (best to put all models in data/models folder)?\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, local_files_only=True)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, local_files_only=True)\n",
    "\n",
    "answers = [get_answer(question, context, tokenizer, model) for question, context in zip(test_data[\"question\"], test_data[\"context\"])]\n",
    "\n",
    "# bertscore\n",
    "results = bertscore.compute(predictions=answers, references=gt_answers, lang=\"en\")\n",
    "print(f\"Bertscore\\nF1: {np.array(results['f1']).mean()}, Precision: {np.array(results['precision']).mean()}, Recall: {np.array(results['recall']).mean()}\")\n",
    "\n",
    "# bleu\n",
    "results = bleu.compute(predictions=answers, references=gt_answers)\n",
    "print(f\"Bleu\\n{results}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small - finetuned - (TODO: train set size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bertscore\n",
      "F1: 0.8447753446442741, Precision: 0.8244254706161362, Recall: 0.8669386165482658\n",
      "Bleu\n",
      "{'bleu': 0.08073612217075588, 'precisions': [0.11964735516372796, 0.08536585365853659, 0.07038123167155426, 0.05910543130990415], 'brevity_penalty': 1.0, 'length_ratio': 5.122580645161291, 'translation_length': 794, 'reference_length': 155}\n"
     ]
    }
   ],
   "source": [
    "model_name = \"mrm8488/t5-base-finetuned-squadv2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "answers = [get_answer(question, context, tokenizer, model) for question, context in zip(test_data[\"question\"], test_data[\"context\"])]\n",
    "\n",
    "# bertscore\n",
    "results = bertscore.compute(predictions=answers, references=gt_answers, lang=\"en\")\n",
    "print(f\"Bertscore\\nF1: {np.array(results['f1']).mean()}, Precision: {np.array(results['precision']).mean()}, Recall: {np.array(results['recall']).mean()}\")\n",
    "\n",
    "# bleu\n",
    "results = bleu.compute(predictions=answers, references=gt_answers)\n",
    "print(f\"Bleu\\n{results}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base - finetuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bertscore\n",
      "F1: 0.8797242726598468, Precision: 0.8563968539237976, Recall: 0.9045447430440358\n",
      "Bleu\n",
      "{'bleu': 0.17734617872420755, 'precisions': [0.2669039145907473, 0.18774703557312253, 0.15555555555555556, 0.12690355329949238], 'brevity_penalty': 1.0, 'length_ratio': 3.6258064516129034, 'translation_length': 562, 'reference_length': 155}\n"
     ]
    }
   ],
   "source": [
    "model_name = f\"{local_models_path}/t5-base-finetuned-squadv2-finetuned-NLB-QA-{year}\" # TODO: path (best to put all models in data/models folder)?\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, local_files_only=True)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, local_files_only=True)\n",
    "\n",
    "answers = [get_answer(question, context, tokenizer, model) for question, context in zip(test_data[\"question\"], test_data[\"context\"])]\n",
    "\n",
    "# bertscore\n",
    "results = bertscore.compute(predictions=answers, references=gt_answers, lang=\"en\")\n",
    "print(f\"Bertscore\\nF1: {np.array(results['f1']).mean()}, Precision: {np.array(results['precision']).mean()}, Recall: {np.array(results['recall']).mean()}\")\n",
    "\n",
    "# bleu\n",
    "results = bleu.compute(predictions=answers, references=gt_answers)\n",
    "print(f\"Bleu\\n{results}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base - finetuned - (TODO: train set size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

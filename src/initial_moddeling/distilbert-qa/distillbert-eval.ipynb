{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast, AutoModelForQuestionAnswering, set_seed\n",
    "from datasets import *\n",
    "import numpy as np\n",
    "import torch\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "\n",
    "local_models_path = '../../data/models/BERT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prediction function\n",
    "def inference_answer(question, context, tokenizer, model):\n",
    "    question = question\n",
    "    context = context\n",
    "    test_feature = tokenizer(\n",
    "        question,\n",
    "        context,\n",
    "        max_length=318\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        outputs = model(torch.tensor([test_feature[\"input_ids\"]]))\n",
    "    start_logits = outputs.start_logits.cpu().numpy()\n",
    "    end_logits = outputs.end_logits.cpu().numpy()\n",
    "    answer_ids = test_feature[\"input_ids\"][np.argmax(\n",
    "        start_logits):np.argmax(end_logits)+1]\n",
    "    return \" \".join(tokenizer.batch_decode(answer_ids))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/Luka/.cache/huggingface/datasets/csv/default-d8382661cd597e83/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "Loading cached split indices for dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-d8382661cd597e83\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-c261d5613d28d856.arrow and C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-d8382661cd597e83\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-e61829c1e4a24b65.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-d8382661cd597e83\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-0b15501cefb41ff7.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-d8382661cd597e83\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-e4de42d02343959f.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-d8382661cd597e83\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-a5add48769f938cd.arrow\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset from file and split it into train and test datasets\n",
    "data_2020_full = load_dataset('csv', data_files=f\"../../data/clean/sustainability-report-2020-squad-format.csv\",\n",
    "                    delimiter=\";\", split='train').train_test_split(test_size=0.3, shuffle=True, seed=SEED)\n",
    "\n",
    "# Reformat the train and test set such as they adhere to the SQuAD format (reading from cvs loads strings not objects as expected)\n",
    "data_2020_full[\"test\"] = data_2020_full[\"test\"].map(\n",
    "    lambda example: ast.literal_eval(example[\"answers\"]))\n",
    "data_2020_full[\"test\"] = data_2020_full[\"test\"].map(lambda example: {\"question\": example[\"question\"], \"context\": example[\"context\"], \"answers\": {\n",
    "                                \"text\": example[\"text\"], \"answer_start\": example[\"answer_start\"]}})\n",
    "# replace all \"\\n\" with \" \" in the context, answers and questions\n",
    "data_2020_full[\"test\"] = data_2020_full[\"test\"].map(lambda example: {\"question\": example[\"question\"].replace(\"\\n\", \" \"), \"context\": example[\"context\"].replace(\"\\n\", \" \"), \"answers\": {\n",
    "                                \"text\": [example[\"answers\"][\"text\"][0].replace(\"\\n\", \" \")], \"answer_start\": example[\"answers\"][\"answer_start\"]}})\n",
    "data_2020_full[\"test\"] = data_2020_full[\"test\"].remove_columns([\"text\", \"answer_start\"])\n",
    "# get ground truth answers\n",
    "test_data_2020_full = data_2020_full[\"test\"]\n",
    "gt_answers_2020_full = [temp[\"answers\"][\"text\"][0] for temp in test_data_2020_full]\n",
    "\n",
    "# squad formatted data\n",
    "references_2020 = [{\"answers\": {\"answer_start\": [answer[\"answer_start\"][0]], \"text\": [answer[\"text\"][0]]}, \"id\": str(id)} for id, answer in zip(data_2020_full[\"test\"][\"id\"], data_2020_full[\"test\"][\"answers\"])]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/Luka/.cache/huggingface/datasets/csv/default-003bb09dc8228b5f/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "Loading cached split indices for dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-003bb09dc8228b5f\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-515ab9eb5e89ae1b.arrow and C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-003bb09dc8228b5f\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-bf44f2d0ce4c658e.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-003bb09dc8228b5f\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-0158f993dabee325.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-003bb09dc8228b5f\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-bee36876181b6213.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-003bb09dc8228b5f\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-74602f82da6cdf7e.arrow\n"
     ]
    }
   ],
   "source": [
    "data_2022_full = load_dataset('csv', data_files=f\"../../data/clean/sustainability-report-2022-squad-format.csv\",\n",
    "                    delimiter=\";\", split='train').train_test_split(test_size=0.3, shuffle=True, seed=SEED)\n",
    "\n",
    "# Reformat the train and test set such as they adhere to the SQuAD format (reading from cvs loads strings not objects as expected)\n",
    "data_2022_full[\"test\"] = data_2022_full[\"test\"].map(\n",
    "    lambda example: ast.literal_eval(example[\"answers\"]))\n",
    "data_2022_full[\"test\"] = data_2022_full[\"test\"].map(lambda example: {\"question\": example[\"question\"], \"context\": example[\"context\"], \"answers\": {\n",
    "                                \"text\": example[\"text\"], \"answer_start\": example[\"answer_start\"]}})\n",
    "# replace all \"\\n\" with \" \" in the context, answers and questions\n",
    "data_2022_full[\"test\"] = data_2022_full[\"test\"].map(lambda example: {\"question\": example[\"question\"].replace(\"\\n\", \" \"), \"context\": example[\"context\"].replace(\"\\n\", \" \"), \"answers\": {\n",
    "                                \"text\": [example[\"answers\"][\"text\"][0].replace(\"\\n\", \" \")], \"answer_start\": example[\"answers\"][\"answer_start\"]}})\n",
    "data_2022_full[\"test\"] = data_2022_full[\"test\"].remove_columns([\"text\", \"answer_start\"])\n",
    "\n",
    "test_data_2022_full = data_2022_full[\"test\"]\n",
    "gt_answers_2022_full = [temp[\"answers\"][\"text\"][0] for temp in test_data_2022_full]\n",
    "\n",
    "# squad formatted data\n",
    "references_2022 = [{\"answers\": {\"answer_start\": [answer[\"answer_start\"][0]], \"text\": [answer[\"text\"][0]]}, \"id\": str(id)} for id, answer in zip(data_2022_full[\"test\"][\"id\"], data_2022_full[\"test\"][\"answers\"])]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2022 handwritten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/Luka/.cache/huggingface/datasets/csv/default-7e0bd965690926b0/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "Loading cached split indices for dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-7e0bd965690926b0\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-9bb74a86dcf2fec4.arrow and C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-7e0bd965690926b0\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-3da9507e622ee022.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-7e0bd965690926b0\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-23042aa7dfb34243.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-7e0bd965690926b0\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-9d063bd963fc89f9.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Luka\\.cache\\huggingface\\datasets\\csv\\default-7e0bd965690926b0\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-96b85517b79b1ac5.arrow\n"
     ]
    }
   ],
   "source": [
    "data_2022_handwritten = load_dataset('csv', data_files=f\"../../data/clean/QA_SR_2022_Expert-squad-format.csv\",\n",
    "                                        delimiter=\";\", split='train').train_test_split(test_size=0.3, shuffle=True, seed=SEED)\n",
    "\n",
    "# Reformat the train and test set such as they adhere to the SQuAD format (reading from cvs loads strings not objects as expected)\n",
    "data_2022_handwritten[\"test\"] = data_2022_handwritten[\"test\"].map(\n",
    "    lambda example: ast.literal_eval(example[\"answers\"]))\n",
    "data_2022_handwritten[\"test\"] = data_2022_handwritten[\"test\"].map(lambda example: {\"question\": example[\"question\"], \"context\": example[\"context\"], \"answers\": {\n",
    "                                \"text\": example[\"text\"], \"answer_start\": example[\"answer_start\"]}})\n",
    "# replace all \"\\n\" with \" \" in the context, answers and questions\n",
    "data_2022_handwritten[\"test\"] = data_2022_handwritten[\"test\"].map(lambda example: {\"question\": example[\"question\"].replace(\"\\n\", \" \"), \"context\": example[\"context\"].replace(\"\\n\", \" \"), \"answers\": {\n",
    "                                \"text\": [example[\"answers\"][\"text\"][0].replace(\"\\n\", \" \")], \"answer_start\": example[\"answers\"][\"answer_start\"]}})\n",
    "data_2022_handwritten[\"test\"] = data_2022_handwritten[\"test\"].remove_columns([\"text\", \"answer_start\"])\n",
    "\n",
    "test_data_2022_handwritten = data_2022_handwritten[\"test\"]\n",
    "gt_answers_2022_handwritten = [temp[\"answers\"][\"text\"][0] for temp in test_data_2022_handwritten]\n",
    "\n",
    "# squad formatted data\n",
    "references_2022_handwritten = [{\"answers\": {\"answer_start\": [answer[\"answer_start\"][0]], \"text\": [answer[\"text\"][0]]}, \"id\": str(id)} for id, answer in enumerate(data_2022_handwritten[\"test\"][\"answers\"])]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIMPLE EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "squad_v2_metric = evaluate.load(\"squad_v2\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bertscore results 2020\n",
      "F1: 0.9191093551261085, Precision: 0.9054063920463834, Recall: 0.9349026339394706\n",
      "Bertscore results 2022\n",
      "F1: 0.888749390562004, Precision: 0.8703976368235651, Recall: 0.9097076999806912\n",
      "Bertscore results 2022 handwritten\n",
      "F1: 0.8637932695840534, Precision: 0.8706748579677782, Recall: 0.8577684540497629\n",
      "Bleu results 2020\n",
      "{'bleu': 0.15141296248186284, 'precisions': [0.29705882352941176, 0.1936619718309859, 0.11934156378600823, 0.07655502392344497], 'brevity_penalty': 1.0, 'length_ratio': 2.193548387096774, 'translation_length': 340, 'reference_length': 155}\n",
      "Bleu results 2022\n",
      "{'bleu': 0.10917104830220783, 'precisions': [0.22283356258596973, 0.13344051446945338, 0.08333333333333333, 0.05732484076433121], 'brevity_penalty': 1.0, 'length_ratio': 2.6340579710144927, 'translation_length': 727, 'reference_length': 276}\n",
      "Bleu results 2022 handwritten\n",
      "{'bleu': 0.1535497194269064, 'precisions': [0.49056603773584906, 0.39285714285714285, 0.3387096774193548, 0.2882882882882883], 'brevity_penalty': 0.41457426837024663, 'length_ratio': 0.5317725752508361, 'translation_length': 159, 'reference_length': 299}\n",
      "Squad_v2 results 2020\n",
      "{'exact': 44.642857142857146, 'f1': 57.656286734314676, 'total': 56, 'HasAns_exact': 44.642857142857146, 'HasAns_f1': 57.656286734314676, 'HasAns_total': 56, 'best_exact': 44.642857142857146, 'best_exact_thresh': 0.0, 'best_f1': 57.656286734314676, 'best_f1_thresh': 0.0}\n",
      "Squad_v2 results 2022\n",
      "{'exact': 31.77570093457944, 'f1': 46.81653897165788, 'total': 107, 'HasAns_exact': 31.77570093457944, 'HasAns_f1': 46.81653897165788, 'HasAns_total': 107, 'best_exact': 31.77570093457944, 'best_exact_thresh': 0.0, 'best_f1': 46.81653897165788, 'best_f1_thresh': 0.0}\n",
      "Squad_v2 results 2022 handwritten\n",
      "{'exact': 15.789473684210526, 'f1': 37.71072570138784, 'total': 19, 'HasAns_exact': 15.789473684210526, 'HasAns_f1': 37.71072570138784, 'HasAns_total': 19, 'best_exact': 15.789473684210526, 'best_exact_thresh': 0.0, 'best_f1': 37.71072570138784, 'best_f1_thresh': 0.0}\n"
     ]
    }
   ],
   "source": [
    "model_name = \"distilbert-base-cased-distilled-squad\"\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(model_name)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "answers_2020 = [inference_answer(data_2020_full[\"test\"][\"question\"][idx], data_2020_full[\"test\"][\"context\"][idx], tokenizer, model) for idx in range(data_2020_full[\"test\"].shape[0])]\n",
    "answers_2020_squad = [{\"id\": str(id), \"prediction_text\": answer, \"no_answer_probability\": 0.} for id, answer in zip(data_2020_full[\"test\"][\"id\"], answers_2020)]\n",
    "answers_2022 = [inference_answer(data_2022_full[\"test\"][\"question\"][idx], data_2022_full[\"test\"][\"context\"][idx], tokenizer, model) for idx in range(data_2022_full[\"test\"].shape[0])]\n",
    "answers_2022_squad = [{\"id\": str(id), \"prediction_text\": answer, \"no_answer_probability\": 0.} for id, answer in zip(data_2022_full[\"test\"][\"id\"], answers_2022)]\n",
    "answers_2022_handwritten = [inference_answer(data_2022_handwritten[\"test\"][\"question\"][idx], data_2022_handwritten[\"test\"][\"context\"][idx], tokenizer, model) for idx in range(data_2022_handwritten[\"test\"].shape[0])]\n",
    "answers_2022_handwritten_squad = [{\"id\": str(id), \"prediction_text\": answer, \"no_answer_probability\": 0.} for id, answer in enumerate(answers_2022_handwritten)]\n",
    "\n",
    "# bertscore\n",
    "results_2020 = bertscore.compute(predictions=answers_2020, references=gt_answers_2020_full, lang=\"en\")\n",
    "results_2022 = bertscore.compute(predictions=answers_2022, references=gt_answers_2022_full, lang=\"en\")\n",
    "results_2022_handwritten = bertscore.compute(predictions=answers_2022_handwritten, references=gt_answers_2022_handwritten, lang=\"en\")\n",
    "print(f\"Bertscore results 2020\\nF1: {np.array(results_2020['f1']).mean()}, Precision: {np.array(results_2020['precision']).mean()}, Recall: {np.array(results_2020['recall']).mean()}\")\n",
    "print(f\"Bertscore results 2022\\nF1: {np.array(results_2022['f1']).mean()}, Precision: {np.array(results_2022['precision']).mean()}, Recall: {np.array(results_2022['recall']).mean()}\")\n",
    "print(f\"Bertscore results 2022 handwritten\\nF1: {np.array(results_2022_handwritten['f1']).mean()}, Precision: {np.array(results_2022_handwritten['precision']).mean()}, Recall: {np.array(results_2022_handwritten['recall']).mean()}\")\n",
    "\n",
    "# bleu\n",
    "results_2020 = bleu.compute(predictions=answers_2020, references=gt_answers_2020_full)\n",
    "results_2022 = bleu.compute(predictions=answers_2022, references=gt_answers_2022_full)\n",
    "results_2022_handwritten = bleu.compute(predictions=answers_2022_handwritten, references=gt_answers_2022_handwritten)\n",
    "print(f\"Bleu results 2020\\n{results_2020}\")\n",
    "print(f\"Bleu results 2022\\n{results_2022}\")\n",
    "print(f\"Bleu results 2022 handwritten\\n{results_2022_handwritten}\")\n",
    "\n",
    "# squad_v2\n",
    "results_2020 = squad_v2_metric.compute(predictions=answers_2020_squad, references=references_2020)\n",
    "results_2022 = squad_v2_metric.compute(predictions=answers_2022_squad, references=references_2022)\n",
    "results_2022_handwritten = squad_v2_metric.compute(predictions=answers_2022_handwritten_squad, references=references_2022_handwritten)\n",
    "print(f\"Squad_v2 results 2020\\n{results_2020}\")\n",
    "print(f\"Squad_v2 results 2022\\n{results_2022}\")\n",
    "print(f\"Squad_v2 results 2022 handwritten\\n{results_2022_handwritten}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10 reports of suspicions of harmful behaviour were categorised as whistle-blow cases',\n",
       " 'ESG risks do not represent a new risk category, but rather one of risk drivers of the existing type of risks, such as credit, liquidity, market and operational risk.',\n",
       " 'organise regular education and training courses',\n",
       " 'NLB Group rejects all forms of bribery and corruption',\n",
       " 'By the year 2022 the NLG Group decreased the number of paper prints by 43% compared to 2019',\n",
       " 'a standardised document that describes ethical business conduct, outlines values, and sets conduct guidelines for relationships with clients, competitors, business partners, state authorities, regulators, shareholders, and internal relationships',\n",
       " 'In NLB Group various (whistleblowing) channels',\n",
       " 'the bank managed to reduce the use of paper by up to 19% in 2022 compared to the previous year',\n",
       " '70%',\n",
       " 'According to ESMS, ESG risk management is considered on three levels',\n",
       " 'Our goal is to raise awareness in among the public of the  importance of physical exercise for preserving health',\n",
       " 'By the year 2030, 75% of all electric energy used in the NLB Group will come from zero-carbon sources.',\n",
       " 'The Bank Association of Slovenia, AmCham, the Chamber of Commerce and Industry of Slovenia, CER-Sustainable Business Network Slovenia, the Sustainable Working Group at the Bank Association of Slovenia and internationally',\n",
       " 'there was one attempted incident of corruption (in NLB d.d.) in 2022',\n",
       " 'refer to: https://whistler.nlb.si/faq-eng',\n",
       " 'In January 2023.',\n",
       " 'only in legally permitted ways',\n",
       " 'Precarious work, gender equality, working environment for people with disabilities, consumer rights and forced labour.',\n",
       " '7 to 8 million  premature deaths']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['10',\n",
       " 'E ##S ##G risks do not',\n",
       " 'organise regular education and training courses',\n",
       " 'unfair , illegal , and de ##tri ##mental to countries with corrupt practices and society in general',\n",
       " '43 %',\n",
       " 'a standard ##ised document that describes ethical business conduct',\n",
       " 'internally and publicly available',\n",
       " '19 %',\n",
       " '70 %',\n",
       " 'three',\n",
       " 'to raise awareness in among the public of the importance of physical exercise for preserving health',\n",
       " 'zero - carbon sources',\n",
       " 'The Bank Association of Slovenia , Am ##C ##ham , the Chamber of Commerce and Industry of Slovenia',\n",
       " 'one',\n",
       " 'https : / / whistle ##r . n ##l ##b . si / f ##aq - en ##g .',\n",
       " 'January 202 ##3',\n",
       " 'only in legally permitted ways',\n",
       " 'companies ad ##here to the constitution and internationally recognized human rights',\n",
       " '7 to 8 million']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(gt_answers_2022_handwritten)\n",
    "display(answers_2022_handwritten)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bertscore results 2020\n",
      "F1: 0.9174406975507736, Precision: 0.902937830558845, Recall: 0.9340311182396752\n",
      "Bertscore results 2022\n",
      "F1: 0.8868488758523888, Precision: 0.8694146097263443, Recall: 0.9062445141444696\n",
      "Bertscore results 2022 handwritten\n",
      "F1: 0.6164656752034238, Precision: 0.6088139759866815, Recall: 0.625061317494041\n",
      "Bleu results 2020\n",
      "{'bleu': 0.1506511045315903, 'precisions': [0.2934472934472934, 0.18983050847457628, 0.11857707509881422, 0.0779816513761468], 'brevity_penalty': 1.0, 'length_ratio': 2.264516129032258, 'translation_length': 351, 'reference_length': 155}\n",
      "Bleu results 2022\n",
      "{'bleu': 0.16115398022423177, 'precisions': [0.2727272727272727, 0.1834862385321101, 0.13174946004319654, 0.10230179028132992], 'brevity_penalty': 1.0, 'length_ratio': 2.351449275362319, 'translation_length': 649, 'reference_length': 276}\n",
      "Bleu results 2022 handwritten\n",
      "{'bleu': 0.09798153053436642, 'precisions': [0.3422818791946309, 0.2814814814814815, 0.2459016393442623, 0.21818181818181817], 'brevity_penalty': 0.365418718329106, 'length_ratio': 0.4983277591973244, 'translation_length': 149, 'reference_length': 299}\n",
      "Squad_v2 results 2020\n",
      "{'exact': 42.857142857142854, 'f1': 56.70390578193373, 'total': 56, 'HasAns_exact': 42.857142857142854, 'HasAns_f1': 56.70390578193373, 'HasAns_total': 56, 'best_exact': 42.857142857142854, 'best_exact_thresh': 0.0, 'best_f1': 56.70390578193373, 'best_f1_thresh': 0.0}\n",
      "Squad_v2 results 2022\n",
      "{'exact': 37.38317757009346, 'f1': 50.04085359784166, 'total': 107, 'HasAns_exact': 37.38317757009346, 'HasAns_f1': 50.04085359784166, 'HasAns_total': 107, 'best_exact': 37.38317757009346, 'best_exact_thresh': 0.0, 'best_f1': 50.04085359784166, 'best_f1_thresh': 0.0}\n",
      "Squad_v2 results 2022 handwritten\n",
      "{'exact': 0.0, 'f1': 19.970603303233194, 'total': 19, 'HasAns_exact': 0.0, 'HasAns_f1': 19.970603303233194, 'HasAns_total': 19, 'best_exact': 0.0, 'best_exact_thresh': 0.0, 'best_f1': 19.970603303233194, 'best_f1_thresh': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    }
   ],
   "source": [
    "model_name_2020 = f\"{local_models_path}/distilbert-base-cased-distilled-squad-finetuned-NLB-QA-2020-full\"\n",
    "tokenizer_2020 = DistilBertTokenizerFast.from_pretrained(model_name_2020)\n",
    "model_2020 = AutoModelForQuestionAnswering.from_pretrained(model_name_2020)\n",
    "\n",
    "model_name_2022 = f\"{local_models_path}/distilbert-base-cased-distilled-squad-finetuned-NLB-QA-2022-full\"\n",
    "tokenizer_2022 = DistilBertTokenizerFast.from_pretrained(model_name_2022)\n",
    "model_2022 = AutoModelForQuestionAnswering.from_pretrained(model_name_2022)\n",
    "\n",
    "model_name_2022_handwritten = f\"{local_models_path}/distilbert-base-cased-distilled-squad-finetuned-NLB-QA-2022-handwritten\"\n",
    "tokenizer_2022_handwritten = DistilBertTokenizerFast.from_pretrained(model_name_2022_handwritten)\n",
    "model_2022_handwritten = AutoModelForQuestionAnswering.from_pretrained(model_name_2022_handwritten)\n",
    "\n",
    "answers_2020 = [inference_answer(data_2020_full[\"test\"][\"question\"][idx], data_2020_full[\"test\"][\"context\"][idx], tokenizer_2020, model_2020) for idx in range(data_2020_full[\"test\"].shape[0])]\n",
    "answers_2020_squad = [{\"id\": str(id), \"prediction_text\": answer, \"no_answer_probability\": 0.} for id, answer in zip(data_2020_full[\"test\"][\"id\"], answers_2020)]\n",
    "\n",
    "answers_2022 = [inference_answer(data_2022_full[\"test\"][\"question\"][idx], data_2022_full[\"test\"][\"context\"][idx], tokenizer_2022, model_2022) for idx in range(data_2022_full[\"test\"].shape[0])]\n",
    "answers_2022_squad = [{\"id\": str(id), \"prediction_text\": answer, \"no_answer_probability\": 0.} for id, answer in zip(data_2022_full[\"test\"][\"id\"], answers_2022)]\n",
    "\n",
    "answers_2022_handwritten = [inference_answer(data_2022_handwritten[\"test\"][\"question\"][idx], data_2022_handwritten[\"test\"][\"context\"][idx], tokenizer_2022_handwritten, model_2022_handwritten) for idx in range(data_2022_handwritten[\"test\"].shape[0])]\n",
    "answers_2022_handwritten_squad = [{\"id\": str(id), \"prediction_text\": answer, \"no_answer_probability\": 0.} for id, answer in enumerate(answers_2022_handwritten)]\n",
    "\n",
    "# bertscore\n",
    "results_2020 = bertscore.compute(predictions=answers_2020, references=gt_answers_2020_full, lang=\"en\")\n",
    "results_2022 = bertscore.compute(predictions=answers_2022, references=gt_answers_2022_full, lang=\"en\")\n",
    "results_2022_handwritten = bertscore.compute(predictions=answers_2022_handwritten, references=gt_answers_2022_handwritten, lang=\"en\")\n",
    "print(f\"Bertscore results 2020\\nF1: {np.array(results_2020['f1']).mean()}, Precision: {np.array(results_2020['precision']).mean()}, Recall: {np.array(results_2020['recall']).mean()}\")\n",
    "print(f\"Bertscore results 2022\\nF1: {np.array(results_2022['f1']).mean()}, Precision: {np.array(results_2022['precision']).mean()}, Recall: {np.array(results_2022['recall']).mean()}\")\n",
    "print(f\"Bertscore results 2022 handwritten\\nF1: {np.array(results_2022_handwritten['f1']).mean()}, Precision: {np.array(results_2022_handwritten['precision']).mean()}, Recall: {np.array(results_2022_handwritten['recall']).mean()}\")\n",
    "\n",
    "# bleu\n",
    "results_2020 = bleu.compute(predictions=answers_2020, references=gt_answers_2020_full)\n",
    "results_2022 = bleu.compute(predictions=answers_2022, references=gt_answers_2022_full)\n",
    "results_2022_handwritten = bleu.compute(predictions=answers_2022_handwritten, references=gt_answers_2022_handwritten)\n",
    "print(f\"Bleu results 2020\\n{results_2020}\")\n",
    "print(f\"Bleu results 2022\\n{results_2022}\")\n",
    "print(f\"Bleu results 2022 handwritten\\n{results_2022_handwritten}\")\n",
    "\n",
    "# squad_v2\n",
    "results_2020 = squad_v2_metric.compute(predictions=answers_2020_squad, references=references_2020)\n",
    "results_2022 = squad_v2_metric.compute(predictions=answers_2022_squad, references=references_2022)\n",
    "results_2022_handwritten = squad_v2_metric.compute(predictions=answers_2022_handwritten_squad, references=references_2022_handwritten)\n",
    "print(f\"Squad_v2 results 2020\\n{results_2020}\")\n",
    "print(f\"Squad_v2 results 2022\\n{results_2022}\")\n",
    "print(f\"Squad_v2 results 2022 handwritten\\n{results_2022_handwritten}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10 reports of suspicions of harmful behaviour were categorised as whistle-blow cases',\n",
       " 'ESG risks do not represent a new risk category, but rather one of risk drivers of the existing type of risks, such as credit, liquidity, market and operational risk.',\n",
       " 'organise regular education and training courses',\n",
       " 'NLB Group rejects all forms of bribery and corruption',\n",
       " 'By the year 2022 the NLG Group decreased the number of paper prints by 43% compared to 2019',\n",
       " 'a standardised document that describes ethical business conduct, outlines values, and sets conduct guidelines for relationships with clients, competitors, business partners, state authorities, regulators, shareholders, and internal relationships',\n",
       " 'In NLB Group various (whistleblowing) channels',\n",
       " 'the bank managed to reduce the use of paper by up to 19% in 2022 compared to the previous year',\n",
       " '70%',\n",
       " 'According to ESMS, ESG risk management is considered on three levels',\n",
       " 'Our goal is to raise awareness in among the public of the  importance of physical exercise for preserving health',\n",
       " 'By the year 2030, 75% of all electric energy used in the NLB Group will come from zero-carbon sources.',\n",
       " 'The Bank Association of Slovenia, AmCham, the Chamber of Commerce and Industry of Slovenia, CER-Sustainable Business Network Slovenia, the Sustainable Working Group at the Bank Association of Slovenia and internationally',\n",
       " 'there was one attempted incident of corruption (in NLB d.d.) in 2022',\n",
       " 'refer to: https://whistler.nlb.si/faq-eng',\n",
       " 'In January 2023.',\n",
       " 'only in legally permitted ways',\n",
       " 'Precarious work, gender equality, working environment for people with disabilities, consumer rights and forced labour.',\n",
       " '7 to 8 million  premature deaths']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['10',\n",
       " 'E ##S ##G risks do not represent a new risk category ,',\n",
       " 'we organise regular education and training courses',\n",
       " 'These forms of actions are unfair , illegal , and de ##tri ##mental to countries with corrupt practices and society in general .',\n",
       " '[CLS]',\n",
       " 'The NL ##B Group Code of Con ##duct is a standard ##ised document that describes ethical business conduct ,',\n",
       " 'internally and publicly available',\n",
       " '',\n",
       " '[CLS]',\n",
       " 'three levels',\n",
       " '[CLS]',\n",
       " '',\n",
       " '',\n",
       " 'On the level of NL ##B Group , there was one attempted incident of corruption ( in NL ##B d . d . ) in 202 ##2 .',\n",
       " '',\n",
       " 'January 202 ##3 .',\n",
       " 'We op ##ti ##mise taxes only in legally permitted ways .',\n",
       " '[CLS]',\n",
       " '']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(gt_answers_2022_handwritten)\n",
    "display(answers_2022_handwritten)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuned  - train set halved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bertscore results 2020\n",
      "F1: 0.916775752391134, Precision: 0.9032407285911697, Recall: 0.9322482645511627\n",
      "Bertscore results 2022\n",
      "F1: 0.8936889589389908, Precision: 0.8761515416831613, Recall: 0.9134437742634355\n",
      "Bleu results 2020\n",
      "{'bleu': 0.1573921816360192, 'precisions': [0.29878048780487804, 0.19852941176470587, 0.12608695652173912, 0.08205128205128205], 'brevity_penalty': 1.0, 'length_ratio': 2.1161290322580646, 'translation_length': 328, 'reference_length': 155}\n",
      "Bleu results 2022\n",
      "{'bleu': 0.1478251376103963, 'precisions': [0.2547584187408492, 0.16782006920415224, 0.11895161290322581, 0.09389671361502347], 'brevity_penalty': 1.0, 'length_ratio': 2.4746376811594204, 'translation_length': 683, 'reference_length': 276}\n",
      "Squad_v2 results 2020\n",
      "{'exact': 42.857142857142854, 'f1': 55.334858162886114, 'total': 56, 'HasAns_exact': 42.857142857142854, 'HasAns_f1': 55.334858162886114, 'HasAns_total': 56, 'best_exact': 42.857142857142854, 'best_exact_thresh': 0.0, 'best_f1': 55.334858162886114, 'best_f1_thresh': 0.0}\n",
      "Squad_v2 results 2022\n",
      "{'exact': 36.44859813084112, 'f1': 48.947543999859164, 'total': 107, 'HasAns_exact': 36.44859813084112, 'HasAns_f1': 48.947543999859164, 'HasAns_total': 107, 'best_exact': 36.44859813084112, 'best_exact_thresh': 0.0, 'best_f1': 48.947543999859164, 'best_f1_thresh': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    }
   ],
   "source": [
    "model_name_2020 = f\"{local_models_path}/distilbert-base-cased-distilled-squad-finetuned-NLB-QA-2020-smaller\"\n",
    "tokenizer_2020 = DistilBertTokenizerFast.from_pretrained(model_name_2020)\n",
    "model_2020 = AutoModelForQuestionAnswering.from_pretrained(model_name_2020)\n",
    "\n",
    "model_name_2022 = f\"{local_models_path}/distilbert-base-cased-distilled-squad-finetuned-NLB-QA-2022-smaller\"\n",
    "tokenizer_2022 = DistilBertTokenizerFast.from_pretrained(model_name_2022)\n",
    "model_2022 = AutoModelForQuestionAnswering.from_pretrained(model_name_2022)\n",
    "\n",
    "answers_2020 = [inference_answer(data_2020_full[\"test\"][\"question\"][idx], data_2020_full[\"test\"][\"context\"][idx], tokenizer_2020, model_2020) for idx in range(data_2020_full[\"test\"].shape[0])]\n",
    "answers_2020_squad = [{\"id\": str(id), \"prediction_text\": answer, \"no_answer_probability\": 0.} for id, answer in zip(data_2020_full[\"test\"][\"id\"], answers_2020)]\n",
    "\n",
    "answers_2022 = [inference_answer(data_2022_full[\"test\"][\"question\"][idx], data_2022_full[\"test\"][\"context\"][idx], tokenizer_2022, model_2022) for idx in range(data_2022_full[\"test\"].shape[0])]\n",
    "answers_2022_squad = [{\"id\": str(id), \"prediction_text\": answer, \"no_answer_probability\": 0.} for id, answer in zip(data_2022_full[\"test\"][\"id\"], answers_2022)]\n",
    "\n",
    "# bertscore\n",
    "results_2020 = bertscore.compute(predictions=answers_2020, references=gt_answers_2020_full, lang=\"en\")\n",
    "results_2022 = bertscore.compute(predictions=answers_2022, references=gt_answers_2022_full, lang=\"en\")\n",
    "print(f\"Bertscore results 2020\\nF1: {np.array(results_2020['f1']).mean()}, Precision: {np.array(results_2020['precision']).mean()}, Recall: {np.array(results_2020['recall']).mean()}\")\n",
    "print(f\"Bertscore results 2022\\nF1: {np.array(results_2022['f1']).mean()}, Precision: {np.array(results_2022['precision']).mean()}, Recall: {np.array(results_2022['recall']).mean()}\")\n",
    "\n",
    "# bleu\n",
    "results_2020 = bleu.compute(predictions=answers_2020, references=gt_answers_2020_full)\n",
    "results_2022 = bleu.compute(predictions=answers_2022, references=gt_answers_2022_full)\n",
    "print(f\"Bleu results 2020\\n{results_2020}\")\n",
    "print(f\"Bleu results 2022\\n{results_2022}\")\n",
    "\n",
    "# squad_v2\n",
    "results_2020 = squad_v2_metric.compute(predictions=answers_2020_squad, references=references_2020)\n",
    "results_2022 = squad_v2_metric.compute(predictions=answers_2022_squad, references=references_2022)\n",
    "print(f\"Squad_v2 results 2020\\n{results_2020}\")\n",
    "print(f\"Squad_v2 results 2022\\n{results_2022}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
